{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/toxic-comment-classification-challenge_gpt2-full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "TU9nxj24x2GC"
      },
      "id": "TU9nxj24x2GC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "import datasets\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 全局配置\n",
        "URLPrefix = \"https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification\"\n",
        "data_dir = 'toxic-comment'\n",
        "DATA_DIR = Path(data_dir)\n",
        "FILENAMES = [\"train.csv\",\"test.csv\",\"test_labels.csv\",\"sample_submission.csv\"]\n",
        "\n",
        "\n",
        "def prepare_csv_list():\n",
        "    \"\"\"下载数据文件\"\"\"\n",
        "    if not DATA_DIR.exists():\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fileName in FILENAMES:\n",
        "        URL = f\"{URLPrefix}/{fileName}\"\n",
        "        DATA_FILE = DATA_DIR / fileName\n",
        "        if not DATA_FILE.exists():\n",
        "            print(f\"⬇️ Downloading {fileName}...\")\n",
        "            with urllib.request.urlopen(URL) as r, open(DATA_FILE, \"wb\") as f:\n",
        "                f.write(r.read())\n",
        "        else:\n",
        "            print(f\"✅ already exists: {fileName}\")\n",
        "\n",
        "\n",
        "def create_dataloaders(data_dir, batch_size, max_length=128):\n",
        "    \"\"\"创建数据加载器\"\"\"\n",
        "    prepare_csv_list()\n",
        "\n",
        "    # 读取数据\n",
        "    train_df = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "    test_df = pd.read_csv(DATA_DIR / \"test.csv\")\n",
        "\n",
        "    # 分割验证集\n",
        "    val_split = int(len(train_df) * 0.8)\n",
        "    val_df = train_df.iloc[val_split:].copy()\n",
        "    train_df = train_df.iloc[:val_split].copy()\n",
        "\n",
        "    print(f\"📊 数据统计: 训练{len(train_df)} 验证{len(val_df)} 测试{len(test_df)}\")\n",
        "\n",
        "    # 初始化分词器\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # 预处理函数\n",
        "    def preprocess_batch(examples):\n",
        "        texts = [str(text).strip() if not pd.isna(text) else \"\" for text in examples['comment_text']]\n",
        "        tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=None)\n",
        "        return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask']}\n",
        "\n",
        "    # 创建数据集\n",
        "    train_dataset = datasets.Dataset.from_pandas(train_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "    val_dataset = datasets.Dataset.from_pandas(val_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "    test_dataset = datasets.Dataset.from_pandas(test_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "\n",
        "    # 添加标签\n",
        "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    def add_labels(examples):\n",
        "        return {'labels': [[examples[col][i] for col in label_cols] for i in range(len(examples['id']))]}\n",
        "\n",
        "    train_dataset = train_dataset.map(add_labels, batched=True)\n",
        "    val_dataset = val_dataset.map(add_labels, batched=True)\n",
        "\n",
        "    # 设置格式\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    # 创建数据加载器 (优化: 增加工作进程和预取因子)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=4, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, test_df['id'].tolist()\n",
        "\n",
        "\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 6):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    hidden = gpt2_out.last_hidden_state  # [B, T, H]\n",
        "    mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n",
        "    masked_sum = (hidden * mask).sum(dim=1)\n",
        "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    pooled = masked_sum / denom\n",
        "    logits = self.classifier(self.dropout(pooled))\n",
        "    return logits\n",
        "\n",
        "# ====== 解冻控制：只解冻顶层若干层 ======\n",
        "def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:\n",
        "    \"\"\"冻结 GPT-2 全部层后，仅解冻顶层 k 个 block（以及最终层归一化与分类头）。\n",
        "\n",
        "    Args:\n",
        "        model: 包含 GPT-2 的分类模型\n",
        "        k: 需要解冻的顶层 block 数量（k<=总层数）。k<=0 表示只训练分类头\n",
        "    \"\"\"\n",
        "    # 冻结全部 GPT-2 参数\n",
        "    for p in model.gpt2.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 仅解冻顶层 k 个 block\n",
        "    if k > 0:\n",
        "        total_blocks = len(model.gpt2.h)\n",
        "        k = min(k, total_blocks)\n",
        "        for block in model.gpt2.h[-k:]:\n",
        "            for p in block.parameters():\n",
        "                p.requires_grad = True\n",
        "        # 同时解冻最终层归一化，有助于适配下游任务\n",
        "        for p in model.gpt2.ln_f.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # 始终训练分类头\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "# 工具类\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"检测可用GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def move_batch_to_device(batch, device, has_labels=True):\n",
        "    \"\"\"将批次数据移动到指定设备\"\"\"\n",
        "    input_ids, attention_mask = batch[:2]\n",
        "    input_ids = input_ids.to(device, non_blocking=True)\n",
        "    attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "    if has_labels and len(batch) > 2:\n",
        "        labels = batch[2].to(device, non_blocking=True)\n",
        "        return input_ids, attention_mask, labels\n",
        "    else:\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "\n",
        "\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    \"\"\"多标签分类准确率\"\"\"\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "\n",
        "def evaluate_model_metrics(net, data_iter, device):\n",
        "    \"\"\"评估模型指标：收集预测结果并计算AUC和F1分数\"\"\"\n",
        "    net.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    # 收集预测结果\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = net(input_ids, attention_mask)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.append(probs.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    probs = torch.cat(all_probs, dim=0).numpy()\n",
        "    labels = torch.cat(all_labels, dim=0).numpy()\n",
        "\n",
        "    # 计算AUC和F1分数\n",
        "    num_labels = probs.shape[1]\n",
        "    best_thresholds, per_label_f1, aucs = [], [], []\n",
        "\n",
        "    for j in range(num_labels):\n",
        "        # AUC计算\n",
        "        try:\n",
        "            auc = roc_auc_score(labels[:, j], probs[:, j])\n",
        "        except Exception:\n",
        "            auc = float('nan')\n",
        "        aucs.append(auc)\n",
        "\n",
        "        # 最佳阈值和F1分数\n",
        "        thrs = np.linspace(0.05, 0.95, 37)\n",
        "        best_t, best_f1 = 0.5, -1.0\n",
        "        y_true, p = labels[:, j], probs[:, j]\n",
        "\n",
        "        for t in thrs:\n",
        "            y_pred = (p >= t).astype(int)\n",
        "            try:\n",
        "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "            except Exception:\n",
        "                f1 = 0.0\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, t\n",
        "\n",
        "        best_thresholds.append(best_t)\n",
        "        per_label_f1.append(best_f1)\n",
        "\n",
        "    # 宏平均\n",
        "    aucs_np = np.array(aucs, dtype=float)\n",
        "    macro_auc = float(np.nanmean(aucs_np)) if np.isnan(aucs_np).any() else float(aucs_np.mean())\n",
        "    macro_f1 = float(np.mean(per_label_f1))\n",
        "\n",
        "    return probs, labels, macro_auc, macro_f1, best_thresholds\n",
        "\n",
        "def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None, progress_bar=None, accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    单个epoch训练 - 混合精度训练 + 学习率调度 + 梯度累积\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # 训练损失总和, 准确数, 样本数\n",
        "\n",
        "    # 使用混合精度训练\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for batch_idx, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "            # 梯度累积缩放\n",
        "            l = l / accumulation_steps\n",
        "\n",
        "        # 反向传播\n",
        "        scaler.scale(l.sum()).backward()\n",
        "\n",
        "        # 梯度累积\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(updater)\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            scaler.step(updater)\n",
        "            scaler.update()\n",
        "            updater.zero_grad()\n",
        "\n",
        "            # 学习率调度（OneCycleLR需要在每个batch后调用）\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum() * accumulation_steps, acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "        cost = time.time() - start_time\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_gpt2_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "            # 使用混合精度推理\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None, patience=2):\n",
        "    \"\"\"\n",
        "    完整训练流程\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # 多GPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_iter_tqdm = tqdm(train_iter,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        # 训练 (添加梯度累积)\n",
        "        train_loss, train_acc = train_gpt2_epoch(\n",
        "            net, train_iter_tqdm, loss, trainer, device, scheduler, train_iter_tqdm, GRADIENT_ACCUMULATION_STEPS\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        val_probs, val_labels, macro_auc, macro_f1, best_thrs = evaluate_model_metrics(net, val_iter, device)\n",
        "\n",
        "        tqdm.write(\n",
        "            f'Epoch {epoch + 1}: '\n",
        "            f'loss {train_loss:.3f}, '\n",
        "            f'train acc {train_acc:.3f}, '\n",
        "            f'val macro AUC {macro_auc:.4f}, '\n",
        "            f'val macro F1 {macro_f1:.4f}, '\n",
        "            f'lr {trainer.param_groups[0][\"lr\"]:.6f}'\n",
        "        )\n",
        "\n",
        "        # Early stopping on macro AUC\n",
        "        if macro_auc > best_auc:\n",
        "            best_auc = macro_auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in net.state_dict().items()}\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                tqdm.write(f\"Early stopping at epoch {epoch + 1} (best macro AUC {best_auc:.4f})\")\n",
        "                break\n",
        "\n",
        "    # print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    if best_state is not None:\n",
        "        net.load_state_dict(best_state)\n",
        "    print(f'Final: best val macro AUC {best_auc:.4f}')\n",
        "\n",
        "\n",
        "# 主执行代码\n",
        "print(\"🚀 启动GPT2多标签分类训练\")\n",
        "\n",
        "# 配置参数 - 可根据需要调整\n",
        "MAX_LENGTH = 128  # 最大序列长度\n",
        "BATCH_SIZE = 64   # 批次大小 (优化: 32 → 64)\n",
        "NUM_EPOCHS = 3    # 训练轮数\n",
        "UNFREEZE_LAYERS = 2  # 解冻的顶层层数\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # 梯度累积步数\n",
        "\n",
        "# 设备配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = BATCH_SIZE if torch.cuda.is_available() else 16\n",
        "num_epochs = NUM_EPOCHS\n",
        "\n",
        "print(f\"📊 配置信息:\")\n",
        "print(f\"  最大序列长度: {MAX_LENGTH}\")\n",
        "print(f\"  批次大小: {batch_size}\")\n",
        "print(f\"  训练轮数: {num_epochs}\")\n",
        "print(f\"  解冻层数: {UNFREEZE_LAYERS}\")\n",
        "print(f\"  梯度累积步数: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "\n",
        "# 数据加载\n",
        "train_iter, val_iter, test_iter, test_ids = create_dataloaders(data_dir, batch_size, MAX_LENGTH)\n",
        "\n",
        "# 模型\n",
        "net = GPT2ClassificationModel()\n",
        "net.to(device)\n",
        "unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_LAYERS)\n",
        "\n",
        "# 模型编译优化 (PyTorch 2.0+)\n",
        "if hasattr(torch, 'compile'):\n",
        "    try:\n",
        "        net = torch.compile(net, mode=\"reduce-overhead\")\n",
        "        print(\"✅ 启用模型编译优化\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  模型编译失败: {e}\")\n",
        "\n",
        "# 梯度检查点优化\n",
        "try:\n",
        "    net.gpt2.gradient_checkpointing_enable()\n",
        "    print(\"✅ 启用梯度检查点\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"可训练参数: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# 优化器\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# 优化器配置\n",
        "base_params = [p for n, p in net.named_parameters() if p.requires_grad and \"classifier\" not in n]\n",
        "head_params = [p for n, p in net.named_parameters() if p.requires_grad and \"classifier\" in n]\n",
        "\n",
        "trainer = AdamW([\n",
        "    {\"params\": base_params, \"lr\": 5e-5},\n",
        "    {\"params\": head_params, \"lr\": 1e-3},\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# 学习率调度\n",
        "num_training_steps = len(train_iter) * num_epochs\n",
        "num_warmup_steps = max(1, int(0.1 * num_training_steps))\n",
        "scheduler = get_linear_schedule_with_warmup(trainer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "# 损失函数\n",
        "train_labels_array = np.array(train_iter.dataset.labels)\n",
        "pos = train_labels_array.sum(axis=0)\n",
        "neg = len(train_labels_array) - pos\n",
        "pos_weight = torch.tensor((neg / (pos + 1e-6)).tolist(), dtype=torch.float).to(device)\n",
        "loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"none\")\n",
        "\n",
        "# 训练\n",
        "net.gpt2.config.use_cache = False\n",
        "try:\n",
        "    net.gpt2.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)\n",
        "print(\"🎉 训练完成!\")\n",
        "\n",
        "\n",
        "import time\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    生成Kaggle提交文件\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"🔮 生成预测结果...\")\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        test_loader_tqdm = tqdm(test_loader,bar_format=\" {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        for i, batch in enumerate(test_loader_tqdm):\n",
        "            try:\n",
        "                input_ids, attention_mask = move_batch_to_device(batch, device, has_labels=False)\n",
        "\n",
        "                # 使用混合精度推理\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                predictions.extend(probs)\n",
        "                cost = time.time() - start_time\n",
        "                test_loader_tqdm.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 在 batch {i} 报错：{repr(e)}\")\n",
        "                print(f\"batch 内容信息:\")\n",
        "                for j, item in enumerate(batch):\n",
        "                    if torch.is_tensor(item):\n",
        "                        print(f\"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}\")\n",
        "                    else:\n",
        "                        print(f\"  非tensor[{j}]: {type(item)}\")\n",
        "                raise e\n",
        "\n",
        "\n",
        "    # 创建提交DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # 保存提交文件\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"💾 提交文件已保存: {output_path}\")\n",
        "    print(f\"📊 预测统计:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: 平均概率 {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# 生成提交文件\n",
        "submission_path = os.path.join(data_dir, 'submission.csv')\n",
        "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
        "print(f\"✅ 提交文件: {submission_path}\")"
      ],
      "metadata": {
        "id": "35KighgnXCUo"
      },
      "id": "35KighgnXCUo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/toxic-comment-classification-challenge_gpt2-full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è®­ç»ƒ"
      ],
      "metadata": {
        "id": "TU9nxj24x2GC"
      },
      "id": "TU9nxj24x2GC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "import datasets\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# å…¨å±€é…ç½®\n",
        "URLPrefix = \"https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification\"\n",
        "data_dir = 'toxic-comment'\n",
        "DATA_DIR = Path(data_dir)\n",
        "FILENAMES = [\"train.csv\",\"test.csv\",\"test_labels.csv\",\"sample_submission.csv\"]\n",
        "\n",
        "\n",
        "def prepare_csv_list():\n",
        "    \"\"\"ä¸‹è½½æ•°æ®æ–‡ä»¶\"\"\"\n",
        "    if not DATA_DIR.exists():\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fileName in FILENAMES:\n",
        "        URL = f\"{URLPrefix}/{fileName}\"\n",
        "        DATA_FILE = DATA_DIR / fileName\n",
        "        if not DATA_FILE.exists():\n",
        "            print(f\"â¬‡ï¸ Downloading {fileName}...\")\n",
        "            with urllib.request.urlopen(URL) as r, open(DATA_FILE, \"wb\") as f:\n",
        "                f.write(r.read())\n",
        "        else:\n",
        "            print(f\"âœ… already exists: {fileName}\")\n",
        "\n",
        "\n",
        "def create_dataloaders(data_dir, batch_size, max_length=128):\n",
        "    \"\"\"åˆ›å»ºæ•°æ®åŠ è½½å™¨\"\"\"\n",
        "    prepare_csv_list()\n",
        "\n",
        "    # è¯»å–æ•°æ®\n",
        "    train_df = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "    test_df = pd.read_csv(DATA_DIR / \"test.csv\")\n",
        "\n",
        "    # åˆ†å‰²éªŒè¯é›†\n",
        "    val_split = int(len(train_df) * 0.8)\n",
        "    val_df = train_df.iloc[val_split:].copy()\n",
        "    train_df = train_df.iloc[:val_split].copy()\n",
        "\n",
        "    print(f\"ğŸ“Š æ•°æ®ç»Ÿè®¡: è®­ç»ƒ{len(train_df)} éªŒè¯{len(val_df)} æµ‹è¯•{len(test_df)}\")\n",
        "\n",
        "    # åˆå§‹åŒ–åˆ†è¯å™¨\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # é¢„å¤„ç†å‡½æ•°\n",
        "    def preprocess_batch(examples):\n",
        "        texts = [str(text).strip() if not pd.isna(text) else \"\" for text in examples['comment_text']]\n",
        "        tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=None)\n",
        "        return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask']}\n",
        "\n",
        "    # åˆ›å»ºæ•°æ®é›†\n",
        "    train_dataset = datasets.Dataset.from_pandas(train_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "    val_dataset = datasets.Dataset.from_pandas(val_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "    test_dataset = datasets.Dataset.from_pandas(test_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)\n",
        "\n",
        "    # æ·»åŠ æ ‡ç­¾\n",
        "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    def add_labels(examples):\n",
        "        return {'labels': [[examples[col][i] for col in label_cols] for i in range(len(examples['id']))]}\n",
        "\n",
        "    train_dataset = train_dataset.map(add_labels, batched=True)\n",
        "    val_dataset = val_dataset.map(add_labels, batched=True)\n",
        "\n",
        "    # è®¾ç½®æ ¼å¼\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (ä¼˜åŒ–: å¢åŠ å·¥ä½œè¿›ç¨‹å’Œé¢„å–å› å­)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=4, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, test_df['id'].tolist()\n",
        "\n",
        "\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 6):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    hidden = gpt2_out.last_hidden_state  # [B, T, H]\n",
        "    mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n",
        "    masked_sum = (hidden * mask).sum(dim=1)\n",
        "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    pooled = masked_sum / denom\n",
        "    logits = self.classifier(self.dropout(pooled))\n",
        "    return logits\n",
        "\n",
        "# ====== è§£å†»æ§åˆ¶ï¼šåªè§£å†»é¡¶å±‚è‹¥å¹²å±‚ ======\n",
        "def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:\n",
        "    \"\"\"å†»ç»“ GPT-2 å…¨éƒ¨å±‚åï¼Œä»…è§£å†»é¡¶å±‚ k ä¸ª blockï¼ˆä»¥åŠæœ€ç»ˆå±‚å½’ä¸€åŒ–ä¸åˆ†ç±»å¤´ï¼‰ã€‚\n",
        "\n",
        "    Args:\n",
        "        model: åŒ…å« GPT-2 çš„åˆ†ç±»æ¨¡å‹\n",
        "        k: éœ€è¦è§£å†»çš„é¡¶å±‚ block æ•°é‡ï¼ˆk<=æ€»å±‚æ•°ï¼‰ã€‚k<=0 è¡¨ç¤ºåªè®­ç»ƒåˆ†ç±»å¤´\n",
        "    \"\"\"\n",
        "    # å†»ç»“å…¨éƒ¨ GPT-2 å‚æ•°\n",
        "    for p in model.gpt2.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # ä»…è§£å†»é¡¶å±‚ k ä¸ª block\n",
        "    if k > 0:\n",
        "        total_blocks = len(model.gpt2.h)\n",
        "        k = min(k, total_blocks)\n",
        "        for block in model.gpt2.h[-k:]:\n",
        "            for p in block.parameters():\n",
        "                p.requires_grad = True\n",
        "        # åŒæ—¶è§£å†»æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºé€‚é…ä¸‹æ¸¸ä»»åŠ¡\n",
        "        for p in model.gpt2.ln_f.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # å§‹ç»ˆè®­ç»ƒåˆ†ç±»å¤´\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "# å·¥å…·ç±»\n",
        "class Accumulator:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        import time\n",
        "        self.time = time\n",
        "        self.start_time = self.time.time()\n",
        "    def stop(self):\n",
        "        return self.time.time() - self.start_time\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"æ£€æµ‹å¯ç”¨GPU\"\"\"\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def move_batch_to_device(batch, device, has_labels=True):\n",
        "    \"\"\"å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\"\"\"\n",
        "    input_ids, attention_mask = batch[:2]\n",
        "    input_ids = input_ids.to(device, non_blocking=True)\n",
        "    attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "    if has_labels and len(batch) > 2:\n",
        "        labels = batch[2].to(device, non_blocking=True)\n",
        "        return input_ids, attention_mask, labels\n",
        "    else:\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "\n",
        "\n",
        "def multilabel_accuracy(y_hat, y):\n",
        "    \"\"\"å¤šæ ‡ç­¾åˆ†ç±»å‡†ç¡®ç‡\"\"\"\n",
        "    predictions = torch.sigmoid(y_hat) > 0.5\n",
        "    y = y.bool()\n",
        "    label_wise_acc = (predictions == y).float().mean()\n",
        "    return label_wise_acc.item()\n",
        "\n",
        "def evaluate_model_metrics(net, data_iter, device):\n",
        "    \"\"\"è¯„ä¼°æ¨¡å‹æŒ‡æ ‡ï¼šæ”¶é›†é¢„æµ‹ç»“æœå¹¶è®¡ç®—AUCå’ŒF1åˆ†æ•°\"\"\"\n",
        "    net.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "\n",
        "    # æ”¶é›†é¢„æµ‹ç»“æœ\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = net(input_ids, attention_mask)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.append(probs.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    probs = torch.cat(all_probs, dim=0).numpy()\n",
        "    labels = torch.cat(all_labels, dim=0).numpy()\n",
        "\n",
        "    # è®¡ç®—AUCå’ŒF1åˆ†æ•°\n",
        "    num_labels = probs.shape[1]\n",
        "    best_thresholds, per_label_f1, aucs = [], [], []\n",
        "\n",
        "    for j in range(num_labels):\n",
        "        # AUCè®¡ç®—\n",
        "        try:\n",
        "            auc = roc_auc_score(labels[:, j], probs[:, j])\n",
        "        except Exception:\n",
        "            auc = float('nan')\n",
        "        aucs.append(auc)\n",
        "\n",
        "        # æœ€ä½³é˜ˆå€¼å’ŒF1åˆ†æ•°\n",
        "        thrs = np.linspace(0.05, 0.95, 37)\n",
        "        best_t, best_f1 = 0.5, -1.0\n",
        "        y_true, p = labels[:, j], probs[:, j]\n",
        "\n",
        "        for t in thrs:\n",
        "            y_pred = (p >= t).astype(int)\n",
        "            try:\n",
        "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "            except Exception:\n",
        "                f1 = 0.0\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, t\n",
        "\n",
        "        best_thresholds.append(best_t)\n",
        "        per_label_f1.append(best_f1)\n",
        "\n",
        "    # å®å¹³å‡\n",
        "    aucs_np = np.array(aucs, dtype=float)\n",
        "    macro_auc = float(np.nanmean(aucs_np)) if np.isnan(aucs_np).any() else float(aucs_np.mean())\n",
        "    macro_f1 = float(np.mean(per_label_f1))\n",
        "\n",
        "    return probs, labels, macro_auc, macro_f1, best_thresholds\n",
        "\n",
        "def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None, progress_bar=None, accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    å•ä¸ªepochè®­ç»ƒ - æ··åˆç²¾åº¦è®­ç»ƒ + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦ç´¯ç§¯\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°\n",
        "\n",
        "    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for batch_idx, batch in enumerate(train_iter):\n",
        "        input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_hat = net(input_ids, attention_mask)\n",
        "            l = loss(y_hat, labels)\n",
        "            # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾\n",
        "            l = l / accumulation_steps\n",
        "\n",
        "        # åå‘ä¼ æ’­\n",
        "        scaler.scale(l.sum()).backward()\n",
        "\n",
        "        # æ¢¯åº¦ç´¯ç§¯\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(updater)\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "            scaler.step(updater)\n",
        "            scaler.update()\n",
        "            updater.zero_grad()\n",
        "\n",
        "            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRéœ€è¦åœ¨æ¯ä¸ªbatchåè°ƒç”¨ï¼‰\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(l.sum() * accumulation_steps, acc * labels.shape[0], labels.shape[0])\n",
        "\n",
        "        cost = time.time() - start_time\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "def evaluate_gpt2_accuracy(net, data_iter, device):\n",
        "    net.eval()\n",
        "    metric = Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            input_ids, attention_mask, labels = move_batch_to_device(batch, device)\n",
        "\n",
        "            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†\n",
        "            with torch.cuda.amp.autocast():\n",
        "                y_hat = net(input_ids, attention_mask)\n",
        "\n",
        "            acc = multilabel_accuracy(y_hat, labels)\n",
        "            metric.add(acc * labels.shape[0], labels.shape[0])\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None, patience=2):\n",
        "    \"\"\"\n",
        "    å®Œæ•´è®­ç»ƒæµç¨‹\n",
        "    \"\"\"\n",
        "    print('training on', devices)\n",
        "\n",
        "    if isinstance(devices, list) and len(devices) > 1:\n",
        "        # å¤šGPU\n",
        "        net = nn.DataParallel(net, device_ids=devices)\n",
        "\n",
        "    device = devices[0] if isinstance(devices, list) else devices\n",
        "    net = net.to(device)\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_iter_tqdm = tqdm(train_iter,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        # è®­ç»ƒ (æ·»åŠ æ¢¯åº¦ç´¯ç§¯)\n",
        "        train_loss, train_acc = train_gpt2_epoch(\n",
        "            net, train_iter_tqdm, loss, trainer, device, scheduler, train_iter_tqdm, GRADIENT_ACCUMULATION_STEPS\n",
        "        )\n",
        "\n",
        "        # éªŒè¯\n",
        "        val_probs, val_labels, macro_auc, macro_f1, best_thrs = evaluate_model_metrics(net, val_iter, device)\n",
        "\n",
        "        tqdm.write(\n",
        "            f'Epoch {epoch + 1}: '\n",
        "            f'loss {train_loss:.3f}, '\n",
        "            f'train acc {train_acc:.3f}, '\n",
        "            f'val macro AUC {macro_auc:.4f}, '\n",
        "            f'val macro F1 {macro_f1:.4f}, '\n",
        "            f'lr {trainer.param_groups[0][\"lr\"]:.6f}'\n",
        "        )\n",
        "\n",
        "        # Early stopping on macro AUC\n",
        "        if macro_auc > best_auc:\n",
        "            best_auc = macro_auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in net.state_dict().items()}\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                tqdm.write(f\"Early stopping at epoch {epoch + 1} (best macro AUC {best_auc:.4f})\")\n",
        "                break\n",
        "\n",
        "    # print(f'Training completed in {timer.stop():.1f} sec')\n",
        "    if best_state is not None:\n",
        "        net.load_state_dict(best_state)\n",
        "    print(f'Final: best val macro AUC {best_auc:.4f}')\n",
        "\n",
        "\n",
        "# ä¸»æ‰§è¡Œä»£ç \n",
        "print(\"ğŸš€ å¯åŠ¨GPT2å¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ\")\n",
        "\n",
        "# é…ç½®å‚æ•° - å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
        "MAX_LENGTH = 128  # æœ€å¤§åºåˆ—é•¿åº¦\n",
        "BATCH_SIZE = 64   # æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–: 32 â†’ 64)\n",
        "NUM_EPOCHS = 3    # è®­ç»ƒè½®æ•°\n",
        "UNFREEZE_LAYERS = 2  # è§£å†»çš„é¡¶å±‚å±‚æ•°\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
        "\n",
        "# è®¾å¤‡é…ç½®\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = BATCH_SIZE if torch.cuda.is_available() else 16\n",
        "num_epochs = NUM_EPOCHS\n",
        "\n",
        "print(f\"ğŸ“Š é…ç½®ä¿¡æ¯:\")\n",
        "print(f\"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_LENGTH}\")\n",
        "print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
        "print(f\"  è®­ç»ƒè½®æ•°: {num_epochs}\")\n",
        "print(f\"  è§£å†»å±‚æ•°: {UNFREEZE_LAYERS}\")\n",
        "print(f\"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "\n",
        "# æ•°æ®åŠ è½½\n",
        "train_iter, val_iter, test_iter, test_ids = create_dataloaders(data_dir, batch_size, MAX_LENGTH)\n",
        "\n",
        "# æ¨¡å‹\n",
        "net = GPT2ClassificationModel()\n",
        "net.to(device)\n",
        "unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_LAYERS)\n",
        "\n",
        "# æ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)\n",
        "if hasattr(torch, 'compile'):\n",
        "    try:\n",
        "        net = torch.compile(net, mode=\"reduce-overhead\")\n",
        "        print(\"âœ… å¯ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}\")\n",
        "\n",
        "# æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–\n",
        "try:\n",
        "    net.gpt2.gradient_checkpointing_enable()\n",
        "    print(\"âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# ä¼˜åŒ–å™¨\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# ä¼˜åŒ–å™¨é…ç½®\n",
        "base_params = [p for n, p in net.named_parameters() if p.requires_grad and \"classifier\" not in n]\n",
        "head_params = [p for n, p in net.named_parameters() if p.requires_grad and \"classifier\" in n]\n",
        "\n",
        "trainer = AdamW([\n",
        "    {\"params\": base_params, \"lr\": 5e-5},\n",
        "    {\"params\": head_params, \"lr\": 1e-3},\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# å­¦ä¹ ç‡è°ƒåº¦\n",
        "num_training_steps = len(train_iter) * num_epochs\n",
        "num_warmup_steps = max(1, int(0.1 * num_training_steps))\n",
        "scheduler = get_linear_schedule_with_warmup(trainer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "# æŸå¤±å‡½æ•°\n",
        "train_labels_array = np.array(train_iter.dataset.labels)\n",
        "pos = train_labels_array.sum(axis=0)\n",
        "neg = len(train_labels_array) - pos\n",
        "pos_weight = torch.tensor((neg / (pos + 1e-6)).tolist(), dtype=torch.float).to(device)\n",
        "loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"none\")\n",
        "\n",
        "# è®­ç»ƒ\n",
        "net.gpt2.config.use_cache = False\n",
        "try:\n",
        "    net.gpt2.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)\n",
        "print(\"ğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
        "\n",
        "\n",
        "import time\n",
        "def generate_submission(model, test_loader, device, test_ids, output_path):\n",
        "    \"\"\"\n",
        "    ç”ŸæˆKaggleæäº¤æ–‡ä»¶\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    print(\"ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...\")\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        test_loader_tqdm = tqdm(test_loader,bar_format=\" {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        for i, batch in enumerate(test_loader_tqdm):\n",
        "            try:\n",
        "                input_ids, attention_mask = move_batch_to_device(batch, device, has_labels=False)\n",
        "\n",
        "                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                predictions.extend(probs)\n",
        "                cost = time.time() - start_time\n",
        "                test_loader_tqdm.set_postfix({\"Cost\": f\"{cost:.2f}s\"})\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ åœ¨ batch {i} æŠ¥é”™ï¼š{repr(e)}\")\n",
        "                print(f\"batch å†…å®¹ä¿¡æ¯:\")\n",
        "                for j, item in enumerate(batch):\n",
        "                    if torch.is_tensor(item):\n",
        "                        print(f\"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}\")\n",
        "                    else:\n",
        "                        print(f\"  étensor[{j}]: {type(item)}\")\n",
        "                raise e\n",
        "\n",
        "\n",
        "    # åˆ›å»ºæäº¤DataFrame\n",
        "    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}\n",
        "    })\n",
        "\n",
        "    # ä¿å­˜æäº¤æ–‡ä»¶\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}\")\n",
        "    print(f\"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:\")\n",
        "    for i, col in enumerate(label_columns):\n",
        "        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)\n",
        "        print(f\"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# ç”Ÿæˆæäº¤æ–‡ä»¶\n",
        "submission_path = os.path.join(data_dir, 'submission.csv')\n",
        "submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)\n",
        "print(f\"âœ… æäº¤æ–‡ä»¶: {submission_path}\")"
      ],
      "metadata": {
        "id": "35KighgnXCUo"
      },
      "id": "35KighgnXCUo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
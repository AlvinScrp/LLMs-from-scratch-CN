import os
import urllib.request
from pathlib import Path
import re
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from transformers import GPT2Tokenizer, GPT2Model, DataCollatorWithPadding
from tqdm import tqdm
import time
import warnings
import datasets

warnings.filterwarnings('ignore')

# ========= æ–‡æœ¬æ¸…æ´—è§„åˆ™ï¼ˆæ¥è‡ªï¼šä¼˜åŒ–æ•°æ®comment_text.mdï¼‰ =========
CHARS_TO_REMOVE = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\nâ€œâ€â€™\'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\Ã—â„¢âˆšÂ²â€”'
# We will filter all characters except alphabet characters and some punctuation
valid_characters = " " + "@$" + "'!?-" + "abcdefghijklmnopqrstuvwxyz" + "abcdefghijklmnopqrstuvwxyz".upper()
valid_characters_ext = valid_characters + "abcdefghijklmnopqrstuvwxyz".upper()

# ç›´æ¥åˆ¤å®šè¯ï¼ˆå½“å‰ä¸ç›´æ¥ç”¨äºæ‰“æ ‡ç­¾ï¼Œä»…ç”¨äºæ¸…æ´—å‚è€ƒï¼‰
TOXIC_WORDS = [
    "poop", "crap", "prick", "twat", "wikipedia", "wiki", "hahahahaha", "lol",
    "bastard", "sluts", "slut", "douchebag", "douche", "blowjob", "nigga", "dumb",
    "jerk", "wanker", "wank", "penis", "motherfucker", "fucker", "fuk", "fucking",
    "fucked", "fuck", "bullshit", "shit", "stupid", "bitches", "bitch", "suck",
    "cunt", "dick", "cocks", "cock", "die", "kill", "gay", "jewish", "jews", "jew",
    "niggers", "nigger", "faggot", "fag", "asshole"
]

# æ˜Ÿå·æ›¿æ¢è¯å¯¹ï¼ˆå‰è€…â†’åè€…ï¼‰
ASTERICKS_WORDS = [
    ('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'),
    ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'),
    ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'),
    ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'),
    ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'),
    ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'),
    ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'),
    ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'),
    ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'),
    ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'),
    ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'),
    ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'),
    ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'),
    ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'),
    ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')
]

# fastText æ‹¼å†™å½’ä¸€
FASTTEXT_MISSPELLINGS = {"'n'balls": 'balls', "-nazi's": 'nazis', 'adminabuse': 'admin abuse', "admins's": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', "blowing's": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', "d'idiots": 'idiots', 'deminazi': 'nazi', 'dftt': "don't feed the troll", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', "g'damn": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', "jackass's": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', "m'igger": 'nigger', "m'iggers": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', "non-nazi's": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', "revertin'im": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', "t'nonsense": 'nonsense', "threatt's": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}

# æ”¶ç¼©è¯å±•å¼€
CONT_PATTERNS = [
    (r'(W|w)on\'t', r'will not'),
    (r'(C|c)an\'t', r'can not'),
    (r'(I|i)\'m', r'i am'),
    (r'(A|a)in\'t', r'is not'),
    (r'(\w+)\'ll', r'\g<1> will'),
    (r'(\w+)n\'t', r'\g<1> not'),
    (r'(\w+)\'ve', r'\g<1> have'),
    (r'(\w+)\'s', r'\g<1> is'),
    (r'(\w+)\'re', r'\g<1> are'),
    (r'(\w+)\'d', r'\g<1> would'),
]

def normalize_text(text: str) -> str:
    if text is None or (isinstance(text, float) and np.isnan(text)):
        return ""
    s = str(text)
    s = s.strip().lower()
    # å¤„ç†ä¸é—´æ–­ç©ºæ ¼ \xa0
    s = s.replace("\xa0", " ")

    # å±•å¼€æ”¶ç¼©è¯
    for pattern, repl in CONT_PATTERNS:
        s = re.sub(pattern, repl, s)

    # æ˜Ÿå·å˜ä½“å½’ä¸€
    for bad, good in ASTERICKS_WORDS:
        s = s.replace(bad, good)

    # fastText æ‹¼å†™å½’ä¸€ï¼ˆæŒ‰è¯è¾¹ç•Œï¼‰
    for bad, good in FASTTEXT_MISSPELLINGS.items():
        s = re.sub(rf'\b{re.escape(bad)}\b', good, s)

    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆæ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä¿ç•™åˆ†è¯è¾¹ç•Œï¼‰
    trans = {ord(c): " " for c in CHARS_TO_REMOVE}
    s = s.translate(trans)

    # å…è®¸å­—ç¬¦ç™½åå•è¿‡æ»¤ï¼ˆä¸åœ¨ç™½åå•çš„å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼‰
    allowed = set(valid_characters_ext)
    s = ''.join(ch if ch in allowed else ' ' for ch in s)

    # åˆå¹¶å¤šç©ºæ ¼
    s = re.sub(r'\s+', ' ', s).strip()
    return s

# å…¨å±€é…ç½®
URLPrefix = "https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification"
data_dir = 'toxic-comment'
DATA_DIR = Path(data_dir)
FILENAMES = ["train.csv","test.csv","test_labels.csv","sample_submission.csv"]


def prepare_csv_list():
    """ä¸‹è½½æ•°æ®æ–‡ä»¶"""
    if not DATA_DIR.exists():
        DATA_DIR.mkdir(parents=True, exist_ok=True)

    for fileName in FILENAMES:
        URL = f"{URLPrefix}/{fileName}"
        DATA_FILE = DATA_DIR / fileName
        if not DATA_FILE.exists():
            print(f"â¬‡ï¸ Downloading {fileName}...")
            with urllib.request.urlopen(URL) as r, open(DATA_FILE, "wb") as f:
                f.write(r.read())
        else:
            print(f"âœ… already exists: {fileName}")


def create_dataloaders(data_dir, batch_size, max_length=128):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    prepare_csv_list()
    
    # è¯»å–æ•°æ®
    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    usecols_train = ['id', 'comment_text'] + label_cols
    usecols_test = ['id', 'comment_text']
    train_df = pd.read_csv(DATA_DIR / "train.csv", usecols=usecols_train)
    test_df = pd.read_csv(DATA_DIR / "test.csv", usecols=usecols_test)
    
    # åˆ†å‰²éªŒè¯é›†
    val_split = int(len(train_df) * 0.8)
    val_df = train_df.iloc[val_split:].copy()
    train_df = train_df.iloc[:val_split].copy()
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: è®­ç»ƒ{len(train_df)} éªŒè¯{len(val_df)} æµ‹è¯•{len(test_df)}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é¢„å¤„ç†å‡½æ•°
    def preprocess_batch(examples):
        texts = [normalize_text(text) for text in examples['comment_text']]
        # åŠ¨æ€ paddingï¼šæ­¤å¤„ä¸å¡«å……ï¼Œåªæˆªæ–­ï¼›ç”± collator åœ¨ DataLoader å†…æŒ‰ batch æœ€é•¿å¡«å……
        tokenized = tokenizer(
            texts,
            truncation=True,
            max_length=max_length,
            padding=False,
            return_tensors=None,
        )
        return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask']}
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¯å¹¶è¡Œæ˜ å°„ï¼‰
    cpu_cnt = os.cpu_count() or 2
    MAP_NUM_PROC = min(4, max(1, cpu_cnt // 2))  # é€‚åº¦å¹¶è¡Œï¼Œé¿å…è¿‡è½½
    train_dataset = datasets.Dataset.from_pandas(train_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=MAP_NUM_PROC)
    val_dataset = datasets.Dataset.from_pandas(val_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=MAP_NUM_PROC)
    test_dataset = datasets.Dataset.from_pandas(test_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=MAP_NUM_PROC)
    
    # æ·»åŠ æ ‡ç­¾
    # æ·»åŠ æ ‡ç­¾
    def add_labels(examples):
        labels = [[float(examples[col][i]) for col in label_cols] for i in range(len(examples['id']))]
        return {'labels': labels}
    
    train_dataset = train_dataset.map(add_labels, batched=True)
    val_dataset = val_dataset.map(add_labels, batched=True)
    
    # ä»…ä¿ç•™æ¨¡å‹éœ€è¦çš„åˆ—ï¼Œé¿å… collator çœ‹åˆ°å­—ç¬¦ä¸²/åˆ—è¡¨ç­‰æ— æ³•è½¬å¼ é‡çš„å­—æ®µ
    keep_train_cols = ['input_ids', 'attention_mask', 'labels']
    keep_val_cols = ['input_ids', 'attention_mask', 'labels']
    keep_test_cols = ['input_ids', 'attention_mask']
    train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in keep_train_cols])
    val_dataset = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in keep_val_cols])
    test_dataset = test_dataset.remove_columns([c for c in test_dataset.column_names if c not in keep_test_cols])

    # ä¸å°†æ•°æ®é›†é¢„å…ˆè½¬æ¢ä¸º torchï¼›äº¤ç”± collator åŠ¨æ€ padding å¹¶è½¬æ¢
    
    # DataCollatorï¼šæŒ‰ batch æœ€é•¿åŠ¨æ€ paddingï¼Œ8 å¯¹é½ä»¥åŠ é€Ÿ Tensor Core
    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest', pad_to_multiple_of=8, return_tensors='pt')

    def collate_fn(features):
        labels = None
        if 'labels' in features[0]:
            labels = [f['labels'] for f in features]
            for f in features:
                f.pop('labels', None)
        batch = collator(features)
        if labels is not None:
            batch['labels'] = torch.tensor(labels, dtype=torch.float)
        return batch

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå•è¿›ç¨‹ï¼Œæ›´ç¨³ï¼‰
    NUM_WORKERS_TRAIN = 0
    NUM_WORKERS_EVAL = 0
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=NUM_WORKERS_TRAIN,
        pin_memory=True,
        drop_last=True,
        collate_fn=collate_fn,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=NUM_WORKERS_EVAL,
        pin_memory=True,
        collate_fn=collate_fn,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=NUM_WORKERS_EVAL,
        pin_memory=True,
        collate_fn=collate_fn,
    )

    print(f"ğŸ§µ DataLoader workers -> train/val/test: 0")
    
    return train_loader, val_loader, test_loader, test_df['id'].tolist()


class GPT2ClassificationModel(nn.Module):
  def __init__(self,num_labels = 6):
    super().__init__()
    self.gpt2 = GPT2Model.from_pretrained('gpt2')
    config = self.gpt2.config
    self.dropout = nn.Dropout(p=0.1)
    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)

  def forward(self,input_ids,attention_mask):
    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)
    hidden = gpt2_out.last_hidden_state  # [B, T, H]
    mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]
    masked_sum = (hidden * mask).sum(dim=1)
    denom = mask.sum(dim=1).clamp(min=1e-6)
    pooled = masked_sum / denom
    logits = self.classifier(self.dropout(pooled))
    return logits

# å†»ç»“æ‰€æœ‰å±‚ï¼Œä»…è§£å†»é¡¶å±‚ k ä¸ª Transformer blockï¼ˆä»¥åŠæœ€ç»ˆå±‚å½’ä¸€åŒ–ä¸åˆ†ç±»å¤´ï¼‰
def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:
    # å…ˆå†»ç»“å…¨éƒ¨ GPT-2 å‚æ•°
    for p in model.gpt2.parameters():
        p.requires_grad = False

    # è§£å†»é¡¶å±‚ k ä¸ª block
    total_blocks = len(model.gpt2.h)
    k = max(0, min(k, total_blocks))
    if k > 0:
        for block in model.gpt2.h[-k:]:
            for p in block.parameters():
                p.requires_grad = True
        # åŒæ—¶è§£å†»æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºé€‚é…ä¸‹æ¸¸ä»»åŠ¡
        for p in model.gpt2.ln_f.parameters():
            p.requires_grad = True

    # å§‹ç»ˆè®­ç»ƒåˆ†ç±»å¤´
    for p in model.classifier.parameters():
        p.requires_grad = True

# å·¥å…·ç±»
class Accumulator:
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]

class Timer:
    pass

def try_all_gpus():
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def move_batch_to_device(batch, device, has_labels=True):
    """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    input_ids = batch['input_ids'].to(device, non_blocking=True)
    attention_mask = batch['attention_mask'].to(device, non_blocking=True)
    
    if has_labels and 'labels' in batch:
        labels = batch['labels'].to(device, non_blocking=True)
        return input_ids, attention_mask, labels
    else:
        return input_ids, attention_mask



def multilabel_accuracy(y_hat, y):
    """å¤šæ ‡ç­¾åˆ†ç±»å‡†ç¡®ç‡"""
    predictions = torch.sigmoid(y_hat) > 0.5
    y = y.bool()
    label_wise_acc = (predictions == y).float().mean()
    return label_wise_acc.item()

def evaluate_model_metrics(net, data_iter, device):
    """è¯„ä¼°ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰å¹¶è¿”å›æ¦‚ç‡ä¸æ ‡ç­¾ï¼Œæ–¹ä¾¿åç»­éœ€è¦æ—¶æ‰©å±•ã€‚"""
    net.eval()
    all_probs, all_labels = [], []
    metric = Accumulator(2)
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)
            with torch.cuda.amp.autocast():
                logits = net(input_ids, attention_mask)
            probs = torch.sigmoid(logits)
            all_probs.append(probs.detach().cpu())
            all_labels.append(labels.detach().cpu())
            # å‡†ç¡®ç‡ï¼ˆ0.5 é˜ˆå€¼ï¼‰
            acc = multilabel_accuracy(logits, labels.float())
            metric.add(acc * labels.shape[0], labels.shape[0])
    probs = torch.cat(all_probs, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()
    val_acc = metric[0] / metric[1]
    return probs, labels, val_acc

def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None, progress_bar=None, accumulation_steps=1, log_interval=50):
    """
    å•ä¸ªepochè®­ç»ƒ - æ··åˆç²¾åº¦è®­ç»ƒ + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦ç´¯ç§¯
    """
    net.train()
    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    start_time = time.time()
    for batch_idx, batch in enumerate(train_iter):
        input_ids, attention_mask, labels = move_batch_to_device(batch, device)

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with torch.cuda.amp.autocast():
            y_hat = net(input_ids, attention_mask)
            # ç¡®ä¿æ ‡ç­¾æ˜¯æµ®ç‚¹ç±»å‹ï¼›ä»…ç”¨äºæŸå¤±çš„æ ‡ç­¾åšå¹³æ»‘ï¼Œå‡†ç¡®ç‡ä½¿ç”¨ç¡¬æ ‡ç­¾
            labels_hard = labels.float()
            labels_for_loss = labels_hard
            if LABEL_SMOOTHING and LABEL_SMOOTHING > 0.0:
                eps = float(LABEL_SMOOTHING)
                labels_for_loss = labels_hard * (1.0 - eps) + 0.5 * eps
            l = loss(y_hat, labels_for_loss)

        # åå‘ä¼ æ’­
        scaler.scale(l.sum()).backward()

        # æ¯ä¸ª batch éƒ½è¿›è¡Œä¸€æ¬¡ä¼˜åŒ–æ­¥
        scaler.unscale_(updater)
        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
        scaler.step(updater)
        scaler.update()
        updater.zero_grad()

        # å­¦ä¹ ç‡è°ƒåº¦ï¼šæ¯ä¸ª batch åè°ƒç”¨
        if scheduler is not None:
            scheduler.step()

        with torch.no_grad():
            acc = multilabel_accuracy(y_hat, labels_hard)
            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])

        cost = time.time() - start_time
        if progress_bar is not None and ((batch_idx + 1) % log_interval == 0):
            progress_bar.set_postfix({"Cost": f"{cost:.2f}s"})

    return metric[0] / metric[2], metric[1] / metric[2]

def evaluate_gpt2_accuracy(net, data_iter, device):
    net.eval()
    metric = Accumulator(2)
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)

            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
            with torch.cuda.amp.autocast():
                y_hat = net(input_ids, attention_mask)

            # ç¡®ä¿æ ‡ç­¾æ˜¯æµ®ç‚¹ç±»å‹
            labels = labels.float()
            acc = multilabel_accuracy(y_hat, labels)
            metric.add(acc * labels.shape[0], labels.shape[0])
    return metric[0] / metric[1]

def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None, patience=3):
    """
    å®Œæ•´è®­ç»ƒæµç¨‹
    """
    print('training on', devices)

    # å¼ºåˆ¶å•å¡ï¼Œç§»é™¤ DataParallel

    device = devices[0] if isinstance(devices, list) else devices
    net = net.to(device)

    best_acc = -1.0
    best_state = None
    best_thresholds = None
    epochs_no_improve = 0
    for epoch in range(num_epochs):
        train_iter_tqdm = tqdm(train_iter,
                            desc=f"Epoch {epoch+1}/{num_epochs}",
                            bar_format="{desc}: {n_fmt}/{total_fmt} {postfix}",
                            mininterval=1.0)

        # è®­ç»ƒ (æ·»åŠ æ¢¯åº¦ç´¯ç§¯)
        train_loss, train_acc = train_gpt2_epoch(
            net, train_iter_tqdm, loss, trainer, device, scheduler, train_iter_tqdm, GRADIENT_ACCUMULATION_STEPS, log_interval=50
        )

        # éªŒè¯ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰
        val_probs, val_labels, val_acc = evaluate_model_metrics(net, val_iter, device)

        tqdm.write(
            f'Epoch {epoch + 1}: '
            f'loss {train_loss:.3f}, '
            f'train acc {train_acc:.3f}, '
            f'val acc {val_acc:.4f}, '
            f'lr {trainer.param_groups[0]["lr"]:.6f}'
        )

        # Early stopping & best checkpoint based on val accuracy
        if val_acc > best_acc:
            best_acc = val_acc
            best_state = {k: v.cpu().clone() for k, v in net.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                tqdm.write(f"Early stopping at epoch {epoch + 1} (best val acc {best_acc:.4f})")
                break

    # print(f'Training completed in {timer.stop():.1f} sec')
    if best_state is not None:
        net.load_state_dict(best_state)
    print(f'Final: best val acc {best_acc:.4f}')
    return None


# ä¸»æ‰§è¡Œä»£ç 
print("ğŸš€ å¯åŠ¨GPT2å¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ")

# é…ç½®å‚æ•° - å¯æ ¹æ®éœ€è¦è°ƒæ•´
MAX_LENGTH = 128  # æœ€å¤§åºåˆ—é•¿åº¦
BATCH_SIZE = 32   # æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–: 32 â†’ 64)
NUM_EPOCHS = 8  # è®­ç»ƒè½®æ•°
UNFREEZE_LAYERS = 4  # è§£å†»çš„é¡¶å±‚å±‚æ•°ï¼ˆæå‡å¯è®­ç»ƒå®¹é‡ï¼‰
GRADIENT_ACCUMULATION_STEPS = 1  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ›´ç¨³çš„æœ‰æ•ˆ batchï¼‰
WARMUP_FRACTION = 0.10   # å­¦ä¹ ç‡warmupå æ¯”ï¼ˆ10%ï¼‰
MIN_LR_FACTOR = 0.10      # å­¦ä¹ ç‡ä¸‹é™=åˆå§‹lrçš„10%
DECAY_POWER = 2.0        # ä½™å¼¦é€€ç«çš„è¿›åº¦å¹‚æ¬¡ï¼ˆ>1 æ”¾æ…¢å‰æœŸä¸‹é™ï¼‰
LABEL_SMOOTHING = 0.05   # BCE æ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼ˆ0~0.1 é€šå¸¸æ›´ç¨³ï¼‰

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
devices_for_training = device  # å¼ºåˆ¶å•å¡è®­ç»ƒ
batch_size = BATCH_SIZE if torch.cuda.is_available() else 16
num_epochs = NUM_EPOCHS

print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_LENGTH}")
print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  è§£å†»å±‚æ•°: {UNFREEZE_LAYERS}")
print(f"  GPU æ•°é‡: {num_gpus}")
print(f"  Warmupå æ¯”: {WARMUP_FRACTION}")
print(f"  LRä¸‹é™æ¯”ä¾‹: {MIN_LR_FACTOR}")
print(f"  è¡°å‡æ›²çº¿å¹‚æ¬¡: {DECAY_POWER}")
print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {GRADIENT_ACCUMULATION_STEPS}")

# æ•°æ®åŠ è½½
train_iter, val_iter, test_iter, test_ids = create_dataloaders(data_dir, batch_size, MAX_LENGTH)

net = GPT2ClassificationModel()
net.to(device)
unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_LAYERS)

# åŠ é€Ÿï¼šTF32 + é«˜æ•ˆæ³¨æ„åŠ›ï¼ˆæ”¯æŒ 30/40 ç³»æ˜¾å¡ï¼‰
USE_EFFICIENT_ATTENTION = True  # True: å¼€å¯ Flash/efficient SDPAï¼›False: ä½¿ç”¨ math SDPA
try:
    if torch.cuda.is_available():
        import torch.backends.cuda as cuda_backends
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        try:
            torch.set_float32_matmul_precision('high')
        except Exception:
            pass

        if USE_EFFICIENT_ATTENTION:
            cuda_backends.enable_flash_sdp(True)
            cuda_backends.enable_mem_efficient_sdp(True)
            cuda_backends.enable_math_sdp(False)
            print("âœ… ä½¿ç”¨ Flash/Efficient SDPA åç«¯")
        else:
            cuda_backends.enable_flash_sdp(False)
            cuda_backends.enable_mem_efficient_sdp(False)
            cuda_backends.enable_math_sdp(True)
            print("âœ… ä½¿ç”¨ Math SDPA åç«¯")
except Exception as e:
    print(f"âš ï¸ SDPA/TF32 è®¾ç½®å¤±è´¥: {e}")


print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}")

# ä¼˜åŒ–å™¨
from torch.optim import AdamW

# ä¼˜åŒ–å™¨é…ç½®
base_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" not in n]
head_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" in n]

try:
    trainer = AdamW([
        {"params": base_params, "lr": 5e-5},   # æ›´ç¨³çš„éª¨å¹²å­¦ä¹ ç‡
        {"params": head_params, "lr": 1e-3},   # æ›´ç¨³çš„åˆ†ç±»å¤´å­¦ä¹ ç‡
    ], weight_decay=0.01, fused=True)
    print("âœ… ä½¿ç”¨ fused AdamW")
except TypeError:
    trainer = AdamW([
        {"params": base_params, "lr": 5e-5},
        {"params": head_params, "lr": 1e-3},
    ], weight_decay=0.01)
    print("â„¹ï¸ å½“å‰ç¯å¢ƒä¸æ”¯æŒ fused AdamWï¼Œå·²å›é€€æ™®é€š AdamW")
# ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = None

# æŸå¤±å‡½æ•° - ä»æ•°æ®é›†ä¸­æå–æ ‡ç­¾
print("ğŸ“Š è®¡ç®—ç±»åˆ«æƒé‡...")
# ä½¿ç”¨æ‰¹é‡è®¿é—®æé«˜æ•ˆç‡
train_labels_array = np.array(train_iter.dataset['labels'])
pos = train_labels_array.sum(axis=0)
neg = len(train_labels_array) - pos
pos_weight = torch.tensor((neg / (pos + 1e-6)).tolist(), dtype=torch.float).to(device)
# é™åˆ¶ pos_weight ä¸Šé™ï¼Œé˜²æ­¢è¿‡å¤§å¯¼è‡´è®­ç»ƒä¸ç¨³
pos_weight = torch.clamp(pos_weight, max=10.0)
loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction="none")
print(f"âœ… ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆ: {pos_weight.cpu().numpy()}")

# è®­ç»ƒ
net.gpt2.config.use_cache = False
# å…³é—­æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥æå‡é€Ÿåº¦ï¼ˆæ˜¾å­˜è¶³å¤Ÿæ—¶å»ºè®®å…³é—­ï¼Œå¯è®¾ä¸º True å†å¼€å¯ï¼‰
USE_GRADIENT_CHECKPOINTING = False
if USE_GRADIENT_CHECKPOINTING:
    try:
        net.gpt2.gradient_checkpointing_enable()
        print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    except Exception:
        pass

best_thresholds = train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices_for_training, scheduler)
print("ğŸ‰ è®­ç»ƒå®Œæˆ!")


import time
def generate_submission(model, test_loader, device, test_ids, output_path, thresholds=None):
    """
    ç”ŸæˆKaggleæäº¤æ–‡ä»¶
    """
    model.eval()
    predictions = []  # list of (batch_size, num_labels) numpy arrays

    print("ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    with torch.no_grad():
        start_time = time.time()
        test_loader_tqdm = tqdm(test_loader, bar_format=" {n_fmt}/{total_fmt} {postfix}", mininterval=1.0)

        for i, batch in enumerate(test_loader_tqdm):
            try:
                input_ids, attention_mask = move_batch_to_device(batch, device, has_labels=False)

                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                with torch.cuda.amp.autocast():
                    logits = model(input_ids, attention_mask)

                probs = torch.sigmoid(logits).cpu().numpy()  # (B, C)
                predictions.append(probs)
                cost = time.time() - start_time
                test_loader_tqdm.set_postfix({"Cost": f"{cost:.2f}s"})
            except Exception as e:
                print(f"âŒ åœ¨ batch {i} æŠ¥é”™ï¼š{repr(e)}")
                print(f"batch å†…å®¹ä¿¡æ¯:")
                for j, item in enumerate(batch):
                    if torch.is_tensor(item):
                        print(f"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}")
                    else:
                        print(f"  étensor[{j}]: {type(item)}")
                raise e

    # æ‹¼æ¥é¢„æµ‹ä¸º (N, C)
    if len(predictions) == 0:
        raise RuntimeError("æœªç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœï¼Œpredictions ä¸ºç©º")
    preds_array = np.vstack(predictions).astype(float)  # (N, C)

    # å¯¹é½é•¿åº¦ï¼Œé˜²æ­¢é•¿åº¦ä¸ä¸€è‡´å¯¼è‡´ç©ºå€¼
    n = min(len(test_ids), preds_array.shape[0])
    if n != len(test_ids) or n != preds_array.shape[0]:
        print(f"âš ï¸  é¢„æµ‹è¡Œæ•°({preds_array.shape[0]})ä¸test_ids({len(test_ids)})ä¸ä¸€è‡´ï¼Œå°†æŒ‰è¾ƒå°é•¿åº¦{n}å¯¹é½")
    test_ids = list(test_ids)[:n]
    preds_array = preds_array[:n]

    # åº”ç”¨é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰ï¼Œå¹¶ä¿è¯ä¸ºfloat
    if thresholds is not None:
        thr = np.array(thresholds, dtype=float).reshape(1, -1)  # (1, C)
        preds_array = (preds_array >= thr).astype(float)

    # æ¸…ç†æ•°å€¼ä¸­çš„NaN/Infï¼Œé¿å…ç©ºå€¼å†™å…¥CSV
    preds_array = np.nan_to_num(preds_array, nan=0.0, posinf=1.0, neginf=0.0)

    # åˆ›å»ºæäº¤DataFrame
    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    data_dict = {'id': test_ids}
    for i, col in enumerate(label_columns):
        data_dict[col] = preds_array[:, i].astype(float)
    submission_df = pd.DataFrame(data_dict)

    # ä¿å­˜æäº¤æ–‡ä»¶
    submission_df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    for i, col in enumerate(label_columns):
        avg_prob = float(preds_array[:, i].mean())
        print(f"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}")

    return submission_df

# ç”Ÿæˆæäº¤æ–‡ä»¶
submission_path = os.path.join(data_dir, 'submission.csv')
submission_df = generate_submission(net, test_iter, device, test_ids, submission_path, thresholds=None)
print(f"âœ… æäº¤æ–‡ä»¶: {submission_path}")
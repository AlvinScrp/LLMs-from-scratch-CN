import os
import urllib.request
from pathlib import Path
import re
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from transformers import GPT2Tokenizer, GPT2Model
from sklearn.metrics import roc_auc_score, f1_score
from tqdm import tqdm
import time
import warnings
import datasets

warnings.filterwarnings('ignore')

# å…¨å±€é…ç½®
URLPrefix = "https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification"
data_dir = 'toxic-comment'
DATA_DIR = Path(data_dir)
FILENAMES = ["train.csv","test.csv","test_labels.csv","sample_submission.csv"]


def prepare_csv_list():
    """ä¸‹è½½æ•°æ®æ–‡ä»¶"""
    if not DATA_DIR.exists():
        DATA_DIR.mkdir(parents=True, exist_ok=True)

    for fileName in FILENAMES:
        URL = f"{URLPrefix}/{fileName}"
        DATA_FILE = DATA_DIR / fileName
        if not DATA_FILE.exists():
            print(f"â¬‡ï¸ Downloading {fileName}...")
            with urllib.request.urlopen(URL) as r, open(DATA_FILE, "wb") as f:
                f.write(r.read())
        else:
            print(f"âœ… already exists: {fileName}")


def create_dataloaders(data_dir, batch_size, max_length=128):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    prepare_csv_list()
    
    # è¯»å–æ•°æ®
    train_df = pd.read_csv(DATA_DIR / "train.csv")
    test_df = pd.read_csv(DATA_DIR / "test.csv")
    
    # åˆ†å‰²éªŒè¯é›†
    val_split = int(len(train_df) * 0.8)
    val_df = train_df.iloc[val_split:].copy()
    train_df = train_df.iloc[:val_split].copy()
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: è®­ç»ƒ{len(train_df)} éªŒè¯{len(val_df)} æµ‹è¯•{len(test_df)}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é¢„å¤„ç†å‡½æ•°
    def preprocess_batch(examples):
        texts = [str(text).strip() if not pd.isna(text) else "" for text in examples['comment_text']]
        tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=None)
        return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask']}
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = datasets.Dataset.from_pandas(train_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    val_dataset = datasets.Dataset.from_pandas(val_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    test_dataset = datasets.Dataset.from_pandas(test_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    
    # æ·»åŠ æ ‡ç­¾
    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    def add_labels(examples):
        return {'labels': [[examples[col][i] for col in label_cols] for i in range(len(examples['id']))]}
    
    train_dataset = train_dataset.map(add_labels, batched=True)
    val_dataset = val_dataset.map(add_labels, batched=True)
    
    # è®¾ç½®æ ¼å¼
    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (ä¼˜åŒ–: å¢åŠ å·¥ä½œè¿›ç¨‹å’Œé¢„å–å› å­)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=4, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)
    
    return train_loader, val_loader, test_loader, test_df['id'].tolist()


class GPT2ClassificationModel(nn.Module):
  def __init__(self,num_labels = 6):
    super().__init__()
    self.gpt2 = GPT2Model.from_pretrained('gpt2')
    config = self.gpt2.config
    self.dropout = nn.Dropout(p=0.1)
    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)

  def forward(self,input_ids,attention_mask):
    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)
    hidden = gpt2_out.last_hidden_state  # [B, T, H]
    mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]
    masked_sum = (hidden * mask).sum(dim=1)
    denom = mask.sum(dim=1).clamp(min=1e-6)
    pooled = masked_sum / denom
    logits = self.classifier(self.dropout(pooled))
    return logits

# ====== è§£å†»æ§åˆ¶ï¼šåªè§£å†»é¡¶å±‚è‹¥å¹²å±‚ ======
def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:
    """å†»ç»“ GPT-2 å…¨éƒ¨å±‚åï¼Œä»…è§£å†»é¡¶å±‚ k ä¸ª blockï¼ˆä»¥åŠæœ€ç»ˆå±‚å½’ä¸€åŒ–ä¸åˆ†ç±»å¤´ï¼‰ã€‚

    Args:
        model: åŒ…å« GPT-2 çš„åˆ†ç±»æ¨¡å‹
        k: éœ€è¦è§£å†»çš„é¡¶å±‚ block æ•°é‡ï¼ˆk<=æ€»å±‚æ•°ï¼‰ã€‚k<=0 è¡¨ç¤ºåªè®­ç»ƒåˆ†ç±»å¤´
    """
    # å†»ç»“å…¨éƒ¨ GPT-2 å‚æ•°
    for p in model.gpt2.parameters():
        p.requires_grad = False

    # ä»…è§£å†»é¡¶å±‚ k ä¸ª block
    if k > 0:
        total_blocks = len(model.gpt2.h)
        k = min(k, total_blocks)
        for block in model.gpt2.h[-k:]:
            for p in block.parameters():
                p.requires_grad = True
        # åŒæ—¶è§£å†»æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºé€‚é…ä¸‹æ¸¸ä»»åŠ¡
        for p in model.gpt2.ln_f.parameters():
            p.requires_grad = True

    # å§‹ç»ˆè®­ç»ƒåˆ†ç±»å¤´
    for p in model.classifier.parameters():
        p.requires_grad = True

# å·¥å…·ç±»
class Accumulator:
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]

class Timer:
    def __init__(self):
        import time
        self.time = time
        self.start_time = self.time.time()
    def stop(self):
        return self.time.time() - self.start_time

def try_all_gpus():
    """æ£€æµ‹å¯ç”¨GPU"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def move_batch_to_device(batch, device, has_labels=True):
    """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    input_ids, attention_mask = batch[:2]
    input_ids = input_ids.to(device, non_blocking=True)
    attention_mask = attention_mask.to(device, non_blocking=True)
    
    if has_labels and len(batch) > 2:
        labels = batch[2].to(device, non_blocking=True)
        return input_ids, attention_mask, labels
    else:
        return input_ids, attention_mask



def multilabel_accuracy(y_hat, y):
    """å¤šæ ‡ç­¾åˆ†ç±»å‡†ç¡®ç‡"""
    predictions = torch.sigmoid(y_hat) > 0.5
    y = y.bool()
    label_wise_acc = (predictions == y).float().mean()
    return label_wise_acc.item()

def evaluate_model_metrics(net, data_iter, device):
    """è¯„ä¼°æ¨¡å‹æŒ‡æ ‡ï¼šæ”¶é›†é¢„æµ‹ç»“æœå¹¶è®¡ç®—AUCå’ŒF1åˆ†æ•°"""
    net.eval()
    all_probs, all_labels = [], []
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)
            
            with torch.cuda.amp.autocast():
                logits = net(input_ids, attention_mask)
            
            probs = torch.sigmoid(logits)
            all_probs.append(probs.detach().cpu())
            all_labels.append(labels.detach().cpu())
    
    probs = torch.cat(all_probs, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()
    
    # è®¡ç®—AUCå’ŒF1åˆ†æ•°
    num_labels = probs.shape[1]
    best_thresholds, per_label_f1, aucs = [], [], []
    
    for j in range(num_labels):
        # AUCè®¡ç®—
        try:
            auc = roc_auc_score(labels[:, j], probs[:, j])
        except Exception:
            auc = float('nan')
        aucs.append(auc)
        
        # æœ€ä½³é˜ˆå€¼å’ŒF1åˆ†æ•°
        thrs = np.linspace(0.05, 0.95, 37)
        best_t, best_f1 = 0.5, -1.0
        y_true, p = labels[:, j], probs[:, j]
        
        for t in thrs:
            y_pred = (p >= t).astype(int)
            try:
                f1 = f1_score(y_true, y_pred, zero_division=0)
            except Exception:
                f1 = 0.0
            if f1 > best_f1:
                best_f1, best_t = f1, t
        
        best_thresholds.append(best_t)
        per_label_f1.append(best_f1)
    
    # å®å¹³å‡
    aucs_np = np.array(aucs, dtype=float)
    macro_auc = float(np.nanmean(aucs_np)) if np.isnan(aucs_np).any() else float(aucs_np.mean())
    macro_f1 = float(np.mean(per_label_f1))
    
    return probs, labels, macro_auc, macro_f1, best_thresholds

def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None, progress_bar=None, accumulation_steps=1):
    """
    å•ä¸ªepochè®­ç»ƒ - æ··åˆç²¾åº¦è®­ç»ƒ + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦ç´¯ç§¯
    """
    net.train()
    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    start_time = time.time()
    for batch_idx, batch in enumerate(train_iter):
        input_ids, attention_mask, labels = move_batch_to_device(batch, device)

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with torch.cuda.amp.autocast():
            y_hat = net(input_ids, attention_mask)
            l = loss(y_hat, labels)
            # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
            l = l / accumulation_steps

        # åå‘ä¼ æ’­
        scaler.scale(l.sum()).backward()

        # æ¢¯åº¦ç´¯ç§¯
        if (batch_idx + 1) % accumulation_steps == 0:
            scaler.unscale_(updater)
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
            scaler.step(updater)
            scaler.update()
            updater.zero_grad()

            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRéœ€è¦åœ¨æ¯ä¸ªbatchåè°ƒç”¨ï¼‰
            if scheduler is not None:
                scheduler.step()

        with torch.no_grad():
            acc = multilabel_accuracy(y_hat, labels)
            metric.add(l.sum() * accumulation_steps, acc * labels.shape[0], labels.shape[0])

        cost = time.time() - start_time
        if progress_bar is not None:
            progress_bar.set_postfix({"Cost": f"{cost:.2f}s"})

    return metric[0] / metric[2], metric[1] / metric[2]

def evaluate_gpt2_accuracy(net, data_iter, device):
    net.eval()
    metric = Accumulator(2)
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)

            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
            with torch.cuda.amp.autocast():
                y_hat = net(input_ids, attention_mask)

            acc = multilabel_accuracy(y_hat, labels)
            metric.add(acc * labels.shape[0], labels.shape[0])
    return metric[0] / metric[1]

def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None, patience=2):
    """
    å®Œæ•´è®­ç»ƒæµç¨‹
    """
    print('training on', devices)

    if isinstance(devices, list) and len(devices) > 1:
        # å¤šGPU
        net = nn.DataParallel(net, device_ids=devices)

    device = devices[0] if isinstance(devices, list) else devices
    net = net.to(device)

    best_auc = -1.0
    best_state = None
    epochs_no_improve = 0
    for epoch in range(num_epochs):
        train_iter_tqdm = tqdm(train_iter,
                            desc=f"Epoch {epoch+1}/{num_epochs}",
                            bar_format="{desc}: {n_fmt}/{total_fmt} {postfix}")

        # è®­ç»ƒ (æ·»åŠ æ¢¯åº¦ç´¯ç§¯)
        train_loss, train_acc = train_gpt2_epoch(
            net, train_iter_tqdm, loss, trainer, device, scheduler, train_iter_tqdm, GRADIENT_ACCUMULATION_STEPS
        )

        # éªŒè¯
        val_probs, val_labels, macro_auc, macro_f1, best_thrs = evaluate_model_metrics(net, val_iter, device)

        tqdm.write(
            f'Epoch {epoch + 1}: '
            f'loss {train_loss:.3f}, '
            f'train acc {train_acc:.3f}, '
            f'val macro AUC {macro_auc:.4f}, '
            f'val macro F1 {macro_f1:.4f}, '
            f'lr {trainer.param_groups[0]["lr"]:.6f}'
        )

        # Early stopping on macro AUC
        if macro_auc > best_auc:
            best_auc = macro_auc
            best_state = {k: v.cpu().clone() for k, v in net.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                tqdm.write(f"Early stopping at epoch {epoch + 1} (best macro AUC {best_auc:.4f})")
                break

    # print(f'Training completed in {timer.stop():.1f} sec')
    if best_state is not None:
        net.load_state_dict(best_state)
    print(f'Final: best val macro AUC {best_auc:.4f}')


# ä¸»æ‰§è¡Œä»£ç 
print("ğŸš€ å¯åŠ¨GPT2å¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ")

# é…ç½®å‚æ•° - å¯æ ¹æ®éœ€è¦è°ƒæ•´
MAX_LENGTH = 128  # æœ€å¤§åºåˆ—é•¿åº¦
BATCH_SIZE = 64   # æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–: 32 â†’ 64)
NUM_EPOCHS = 3    # è®­ç»ƒè½®æ•°
UNFREEZE_LAYERS = 2  # è§£å†»çš„é¡¶å±‚å±‚æ•°
GRADIENT_ACCUMULATION_STEPS = 2  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
batch_size = BATCH_SIZE if torch.cuda.is_available() else 16
num_epochs = NUM_EPOCHS

print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_LENGTH}")
print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  è§£å†»å±‚æ•°: {UNFREEZE_LAYERS}")
print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {GRADIENT_ACCUMULATION_STEPS}")

# æ•°æ®åŠ è½½
train_iter, val_iter, test_iter, test_ids = create_dataloaders(data_dir, batch_size, MAX_LENGTH)

# æ¨¡å‹
net = GPT2ClassificationModel()
net.to(device)
unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_LAYERS)

# æ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
if hasattr(torch, 'compile'):
    try:
        net = torch.compile(net, mode="reduce-overhead")
        print("âœ… å¯ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

# æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–
try:
    net.gpt2.gradient_checkpointing_enable()
    print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
except Exception:
    pass

print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}")

# ä¼˜åŒ–å™¨
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

# ä¼˜åŒ–å™¨é…ç½®
base_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" not in n]
head_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" in n]

trainer = AdamW([
    {"params": base_params, "lr": 5e-5},
    {"params": head_params, "lr": 1e-3},
], weight_decay=0.01)

# å­¦ä¹ ç‡è°ƒåº¦
num_training_steps = len(train_iter) * num_epochs
num_warmup_steps = max(1, int(0.1 * num_training_steps))
scheduler = get_linear_schedule_with_warmup(trainer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# æŸå¤±å‡½æ•°
train_labels_array = np.array(train_iter.dataset.labels)
pos = train_labels_array.sum(axis=0)
neg = len(train_labels_array) - pos
pos_weight = torch.tensor((neg / (pos + 1e-6)).tolist(), dtype=torch.float).to(device)
loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction="none")

# è®­ç»ƒ
net.gpt2.config.use_cache = False
try:
    net.gpt2.gradient_checkpointing_enable()
except Exception:
    pass

train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)
print("ğŸ‰ è®­ç»ƒå®Œæˆ!")


import time
def generate_submission(model, test_loader, device, test_ids, output_path):
    """
    ç”ŸæˆKaggleæäº¤æ–‡ä»¶
    """
    model.eval()
    predictions = []

    print("ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    with torch.no_grad():
        start_time = time.time()
        test_loader_tqdm = tqdm(test_loader,bar_format=" {n_fmt}/{total_fmt} {postfix}")

        for i, batch in enumerate(test_loader_tqdm):
            try:
                input_ids, attention_mask = move_batch_to_device(batch, device, has_labels=False)

                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                with torch.cuda.amp.autocast():
                    logits = model(input_ids, attention_mask)

                probs = torch.sigmoid(logits).cpu().numpy()
                predictions.extend(probs)
                cost = time.time() - start_time
                test_loader_tqdm.set_postfix({"Cost": f"{cost:.2f}s"})
            except Exception as e:
                print(f"âŒ åœ¨ batch {i} æŠ¥é”™ï¼š{repr(e)}")
                print(f"batch å†…å®¹ä¿¡æ¯:")
                for j, item in enumerate(batch):
                    if torch.is_tensor(item):
                        print(f"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}")
                    else:
                        print(f"  étensor[{j}]: {type(item)}")
                raise e


    # åˆ›å»ºæäº¤DataFrame
    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    submission_df = pd.DataFrame({
        'id': test_ids,
        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}
    })

    # ä¿å­˜æäº¤æ–‡ä»¶
    submission_df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    for i, col in enumerate(label_columns):
        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)
        print(f"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}")

    return submission_df

# ç”Ÿæˆæäº¤æ–‡ä»¶
submission_path = os.path.join(data_dir, 'submission.csv')
submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)
print(f"âœ… æäº¤æ–‡ä»¶: {submission_path}")
import os
import math
import urllib.request
from pathlib import Path
import re
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from transformers import GPT2Tokenizer, GPT2Model
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve
from tqdm import tqdm
import time
import warnings
import datasets

warnings.filterwarnings('ignore')

# å…¨å±€é…ç½®
URLPrefix = "https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification"
data_dir = 'toxic-comment'
DATA_DIR = Path(data_dir)
FILENAMES = ["train.csv","test.csv","test_labels.csv","sample_submission.csv"]


def prepare_csv_list():
    """ä¸‹è½½æ•°æ®æ–‡ä»¶"""
    if not DATA_DIR.exists():
        DATA_DIR.mkdir(parents=True, exist_ok=True)

    for fileName in FILENAMES:
        URL = f"{URLPrefix}/{fileName}"
        DATA_FILE = DATA_DIR / fileName
        if not DATA_FILE.exists():
            print(f"â¬‡ï¸ Downloading {fileName}...")
            with urllib.request.urlopen(URL) as r, open(DATA_FILE, "wb") as f:
                f.write(r.read())
        else:
            print(f"âœ… already exists: {fileName}")


def create_dataloaders(data_dir, batch_size, max_length=128):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    prepare_csv_list()
    
    # è¯»å–æ•°æ®
    train_df = pd.read_csv(DATA_DIR / "train.csv")
    test_df = pd.read_csv(DATA_DIR / "test.csv")
    
    # åˆ†å‰²éªŒè¯é›†
    val_split = int(len(train_df) * 0.8)
    val_df = train_df.iloc[val_split:].copy()
    train_df = train_df.iloc[:val_split].copy()
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: è®­ç»ƒ{len(train_df)} éªŒè¯{len(val_df)} æµ‹è¯•{len(test_df)}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é¢„å¤„ç†å‡½æ•°
    def preprocess_batch(examples):
        texts = [str(text).strip() if not pd.isna(text) else "" for text in examples['comment_text']]
        tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=None)
        return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask']}
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = datasets.Dataset.from_pandas(train_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    val_dataset = datasets.Dataset.from_pandas(val_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    test_dataset = datasets.Dataset.from_pandas(test_df).map(preprocess_batch, batched=True, batch_size=1000, num_proc=4)
    
    # æ·»åŠ æ ‡ç­¾
    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    def add_labels(examples):
        labels = [[float(examples[col][i]) for col in label_cols] for i in range(len(examples['id']))]
        return {'labels': labels}
    
    train_dataset = train_dataset.map(add_labels, batched=True)
    val_dataset = val_dataset.map(add_labels, batched=True)
    
    # è®¾ç½®æ ¼å¼
    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (ä¼˜åŒ–: å¢åŠ å·¥ä½œè¿›ç¨‹å’Œé¢„å–å› å­)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True, prefetch_factor=4, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)
    
    return train_loader, val_loader, test_loader, test_df['id'].tolist()


class GPT2ClassificationModel(nn.Module):
  def __init__(self,num_labels = 6):
    super().__init__()
    self.gpt2 = GPT2Model.from_pretrained('gpt2')
    config = self.gpt2.config
    self.dropout = nn.Dropout(p=0.1)
    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)

  def forward(self,input_ids,attention_mask):
    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)
    hidden = gpt2_out.last_hidden_state  # [B, T, H]
    mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]
    masked_sum = (hidden * mask).sum(dim=1)
    denom = mask.sum(dim=1).clamp(min=1e-6)
    pooled = masked_sum / denom
    logits = self.classifier(self.dropout(pooled))
    return logits

# ====== è§£å†»æ§åˆ¶ï¼šåªè§£å†»é¡¶å±‚è‹¥å¹²å±‚ ======
def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:
    """å†»ç»“ GPT-2 å…¨éƒ¨å±‚åï¼Œä»…è§£å†»é¡¶å±‚ k ä¸ª blockï¼ˆä»¥åŠæœ€ç»ˆå±‚å½’ä¸€åŒ–ä¸åˆ†ç±»å¤´ï¼‰ã€‚

    Args:
        model: åŒ…å« GPT-2 çš„åˆ†ç±»æ¨¡å‹
        k: éœ€è¦è§£å†»çš„é¡¶å±‚ block æ•°é‡ï¼ˆk<=æ€»å±‚æ•°ï¼‰ã€‚k<=0 è¡¨ç¤ºåªè®­ç»ƒåˆ†ç±»å¤´
    """
    # å†»ç»“å…¨éƒ¨ GPT-2 å‚æ•°
    for p in model.gpt2.parameters():
        p.requires_grad = False

    # ä»…è§£å†»é¡¶å±‚ k ä¸ª block
    if k > 0:
        total_blocks = len(model.gpt2.h)
        k = min(k, total_blocks)
        for block in model.gpt2.h[-k:]:
            for p in block.parameters():
                p.requires_grad = True
        # åŒæ—¶è§£å†»æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºé€‚é…ä¸‹æ¸¸ä»»åŠ¡
        for p in model.gpt2.ln_f.parameters():
            p.requires_grad = True

    # å§‹ç»ˆè®­ç»ƒåˆ†ç±»å¤´
    for p in model.classifier.parameters():
        p.requires_grad = True

# å·¥å…·ç±»
class Accumulator:
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]

class Timer:
    def __init__(self):
        import time
        self.time = time
        self.start_time = self.time.time()
    def stop(self):
        return self.time.time() - self.start_time

def try_all_gpus():
    """æ£€æµ‹å¯ç”¨GPU"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def move_batch_to_device(batch, device, has_labels=True):
    """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    input_ids = batch['input_ids'].to(device, non_blocking=True)
    attention_mask = batch['attention_mask'].to(device, non_blocking=True)
    
    if has_labels and 'labels' in batch:
        labels = batch['labels'].to(device, non_blocking=True)
        return input_ids, attention_mask, labels
    else:
        return input_ids, attention_mask



def multilabel_accuracy(y_hat, y):
    """å¤šæ ‡ç­¾åˆ†ç±»å‡†ç¡®ç‡"""
    predictions = torch.sigmoid(y_hat) > 0.5
    y = y.bool()
    label_wise_acc = (predictions == y).float().mean()
    return label_wise_acc.item()

def evaluate_model_metrics(net, data_iter, device):
    """è¯„ä¼°æ¨¡å‹æŒ‡æ ‡ï¼šæ”¶é›†é¢„æµ‹ç»“æœå¹¶è®¡ç®—AUCå’ŒF1åˆ†æ•°"""
    net.eval()
    all_probs, all_labels = [], []
    
    # æ”¶é›†é¢„æµ‹ç»“æœ
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)
            
            with torch.cuda.amp.autocast():
                logits = net(input_ids, attention_mask)
            
            probs = torch.sigmoid(logits)
            all_probs.append(probs.detach().cpu())
            all_labels.append(labels.detach().cpu())
    
    probs = torch.cat(all_probs, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()
    
    # è®¡ç®—AUCå’ŒF1åˆ†æ•°
    num_labels = probs.shape[1]
    best_thresholds, per_label_f1, aucs = [], [], []
    
    for j in range(num_labels):
        # AUCè®¡ç®—
        try:
            auc = roc_auc_score(labels[:, j], probs[:, j])
        except Exception:
            auc = float('nan')
        aucs.append(auc)

        # ä½¿ç”¨ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿å¯»æ‰¾èƒ½æœ€å¤§åŒ–F1çš„é˜ˆå€¼ï¼ˆæ¯”å›ºå®šç½‘æ ¼æ›´ç²¾ç»†ï¼‰
        y_true, p = labels[:, j], probs[:, j]
        try:
            precision, recall, pr_thresholds = precision_recall_curve(y_true, p)
            f1s = (2 * precision * recall) / (precision + recall + 1e-8)
            best_idx = int(np.nanargmax(f1s))
            best_f1 = float(f1s[best_idx])
            # pr_thresholdsé•¿åº¦ä¸ºlen(precision)-1ï¼Œä¸precision/recallå¯¹é½
            if best_idx == 0:
                best_t = float(0.5)  # å½“best_idxä¸º0æ—¶é˜ˆå€¼æœªå®šä¹‰ï¼Œå›é€€åˆ°0.5
            else:
                best_t = float(pr_thresholds[best_idx - 1])
        except Exception:
            best_t, best_f1 = 0.5, 0.0

        best_thresholds.append(best_t)
        per_label_f1.append(best_f1)
    
    # å®å¹³å‡
    aucs_np = np.array(aucs, dtype=float)
    macro_auc = float(np.nanmean(aucs_np)) if np.isnan(aucs_np).any() else float(aucs_np.mean())
    macro_f1 = float(np.mean(per_label_f1))
    
    return probs, labels, macro_auc, macro_f1, best_thresholds

def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None, progress_bar=None, accumulation_steps=1, log_interval=50):
    """
    å•ä¸ªepochè®­ç»ƒ - æ··åˆç²¾åº¦è®­ç»ƒ + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦ç´¯ç§¯
    """
    net.train()
    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    start_time = time.time()
    for batch_idx, batch in enumerate(train_iter):
        input_ids, attention_mask, labels = move_batch_to_device(batch, device)

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with torch.cuda.amp.autocast():
            y_hat = net(input_ids, attention_mask)
            # ç¡®ä¿æ ‡ç­¾æ˜¯æµ®ç‚¹ç±»å‹
            labels = labels.float()
            l = loss(y_hat, labels)
            # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
            l = l / accumulation_steps

        # åå‘ä¼ æ’­
        scaler.scale(l.sum()).backward()

        # æ¢¯åº¦ç´¯ç§¯
        if (batch_idx + 1) % accumulation_steps == 0:
            scaler.unscale_(updater)
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
            scaler.step(updater)
            scaler.update()
            updater.zero_grad()

            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRéœ€è¦åœ¨æ¯ä¸ªbatchåè°ƒç”¨ï¼‰
            if scheduler is not None:
                scheduler.step()

        with torch.no_grad():
            acc = multilabel_accuracy(y_hat, labels)
            metric.add(l.sum() * accumulation_steps, acc * labels.shape[0], labels.shape[0])

        cost = time.time() - start_time
        if progress_bar is not None and ((batch_idx + 1) % log_interval == 0):
            progress_bar.set_postfix({"Cost": f"{cost:.2f}s"})

    return metric[0] / metric[2], metric[1] / metric[2]

def evaluate_gpt2_accuracy(net, data_iter, device):
    net.eval()
    metric = Accumulator(2)
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = move_batch_to_device(batch, device)

            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
            with torch.cuda.amp.autocast():
                y_hat = net(input_ids, attention_mask)

            # ç¡®ä¿æ ‡ç­¾æ˜¯æµ®ç‚¹ç±»å‹
            labels = labels.float()
            acc = multilabel_accuracy(y_hat, labels)
            metric.add(acc * labels.shape[0], labels.shape[0])
    return metric[0] / metric[1]

def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None, patience=3):
    """
    å®Œæ•´è®­ç»ƒæµç¨‹
    """
    print('training on', devices)

    if isinstance(devices, list) and len(devices) > 1:
        # å¤šGPU
        net = nn.DataParallel(net, device_ids=devices)

    device = devices[0] if isinstance(devices, list) else devices
    net = net.to(device)

    best_f1 = -1.0
    best_state = None
    best_thresholds = None
    epochs_no_improve = 0
    for epoch in range(num_epochs):
        train_iter_tqdm = tqdm(train_iter,
                            desc=f"Epoch {epoch+1}/{num_epochs}",
                            bar_format="{desc}: {n_fmt}/{total_fmt} {postfix}",
                            mininterval=1.0)

        # è®­ç»ƒ (æ·»åŠ æ¢¯åº¦ç´¯ç§¯)
        train_loss, train_acc = train_gpt2_epoch(
            net, train_iter_tqdm, loss, trainer, device, scheduler, train_iter_tqdm, GRADIENT_ACCUMULATION_STEPS, log_interval=50
        )

        # éªŒè¯
        val_probs, val_labels, macro_auc, macro_f1, best_thrs = evaluate_model_metrics(net, val_iter, device)

        tqdm.write(
            f'Epoch {epoch + 1}: '
            f'loss {train_loss:.3f}, '
            f'train acc {train_acc:.3f}, '
            f'val macro AUC {macro_auc:.4f}, '
            f'val macro F1 {macro_f1:.4f}, '
            f'lr {trainer.param_groups[0]["lr"]:.6f}'
        )

        # Early stopping & best checkpoint based on macro F1
        if macro_f1 > best_f1:
            best_f1 = macro_f1
            best_state = {k: v.cpu().clone() for k, v in net.state_dict().items()}
            best_thresholds = best_thrs
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                tqdm.write(f"Early stopping at epoch {epoch + 1} (best macro F1 {best_f1:.4f})")
                break

    # print(f'Training completed in {timer.stop():.1f} sec')
    if best_state is not None:
        net.load_state_dict(best_state)
    print(f'Final: best val macro F1 {best_f1:.4f}')
    return best_thresholds


# ä¸»æ‰§è¡Œä»£ç 
print("ğŸš€ å¯åŠ¨GPT2å¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ")

# é…ç½®å‚æ•° - å¯æ ¹æ®éœ€è¦è°ƒæ•´
MAX_LENGTH = 128  # æœ€å¤§åºåˆ—é•¿åº¦
BATCH_SIZE = 64   # æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–: 32 â†’ 64)
NUM_EPOCHS = 10  # è®­ç»ƒè½®æ•°
UNFREEZE_LAYERS = 8  # è§£å†»çš„é¡¶å±‚å±‚æ•°ï¼ˆè¿›ä¸€æ­¥æå‡å¯è®­ç»ƒå®¹é‡ï¼‰
GRADIENT_ACCUMULATION_STEPS = 1  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
WARMUP_FRACTION = 0.05   # å­¦ä¹ ç‡warmupå æ¯”ï¼ˆ5%ï¼‰
MIN_LR_FACTOR = 0.2      # å­¦ä¹ ç‡ä¸‹é™=åˆå§‹lrçš„20%ï¼ˆæ”¾æ…¢åæœŸè¡°å‡ï¼‰
DECAY_POWER = 2.0        # ä½™å¼¦é€€ç«çš„è¿›åº¦å¹‚æ¬¡ï¼ˆ>1 æ”¾æ…¢å‰æœŸä¸‹é™ï¼‰

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
batch_size = BATCH_SIZE if torch.cuda.is_available() else 16
num_epochs = NUM_EPOCHS

print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_LENGTH}")
print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  è§£å†»å±‚æ•°: {UNFREEZE_LAYERS}")
print(f"  Warmupå æ¯”: {WARMUP_FRACTION}")
print(f"  LRä¸‹é™æ¯”ä¾‹: {MIN_LR_FACTOR}")
print(f"  è¡°å‡æ›²çº¿å¹‚æ¬¡: {DECAY_POWER}")
print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {GRADIENT_ACCUMULATION_STEPS}")

# æ•°æ®åŠ è½½
train_iter, val_iter, test_iter, test_ids = create_dataloaders(data_dir, batch_size, MAX_LENGTH)

# æ¨¡å‹
net = GPT2ClassificationModel()
net.to(device)
unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_LAYERS)

# æ¨¡å‹ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
if hasattr(torch, 'compile'):
    try:
        net = torch.compile(net, mode="reduce-overhead")
        print("âœ… å¯ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

# æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–
try:
    net.gpt2.gradient_checkpointing_enable()
    print("âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
except Exception:
    pass

print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}")

# ä¼˜åŒ–å™¨
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR

# ä¼˜åŒ–å™¨é…ç½®
base_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" not in n]
head_params = [p for n, p in net.named_parameters() if p.requires_grad and "classifier" in n]

trainer = AdamW([
    {"params": base_params, "lr": 1e-4},   # æé«˜åŸºåº§å­¦ä¹ ç‡
    {"params": head_params, "lr": 2e-3},   # æé«˜åˆ†ç±»å¤´å­¦ä¹ ç‡
], weight_decay=0.01)

# å­¦ä¹ ç‡è°ƒåº¦ï¼šWarmup + ä½™å¼¦é€€ç«åˆ°ä¸‹é™ï¼ˆæŒ‰ä¼˜åŒ–å™¨stepæ¬¡æ•°è®¡ç®—ï¼‰
total_optimizer_steps = max(1, (len(train_iter) * num_epochs) // max(1, GRADIENT_ACCUMULATION_STEPS))
warmup_steps = max(1, int(WARMUP_FRACTION * total_optimizer_steps))

def lr_lambda(current_step: int) -> float:
    # warmupé˜¶æ®µï¼šçº¿æ€§ä»0â†’1
    if current_step < warmup_steps:
        return float(current_step) / float(max(1, warmup_steps))
    # ä½™å¼¦é€€ç«é˜¶æ®µï¼šä»1é€€ç«åˆ°MIN_LR_FACTOR
    progress = (current_step - warmup_steps) / float(max(1, total_optimizer_steps - warmup_steps))
    progress = min(1.0, max(0.0, progress)) ** DECAY_POWER  # å¹‚æ¬¡å¡‘å½¢ï¼š>1 æ”¾æ…¢å‰æœŸä¸‹é™
    cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
    return float(MIN_LR_FACTOR + (1.0 - MIN_LR_FACTOR) * cosine)

scheduler = LambdaLR(trainer, lr_lambda)

# æŸå¤±å‡½æ•° - ä»æ•°æ®é›†ä¸­æå–æ ‡ç­¾
print("ğŸ“Š è®¡ç®—ç±»åˆ«æƒé‡...")
# ä½¿ç”¨æ‰¹é‡è®¿é—®æé«˜æ•ˆç‡
train_labels_array = np.array(train_iter.dataset['labels'])
pos = train_labels_array.sum(axis=0)
neg = len(train_labels_array) - pos
pos_weight = torch.tensor((neg / (pos + 1e-6)).tolist(), dtype=torch.float).to(device)
loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction="none")
print(f"âœ… ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆ: {pos_weight.cpu().numpy()}")

# è®­ç»ƒ
net.gpt2.config.use_cache = False
try:
    net.gpt2.gradient_checkpointing_enable()
except Exception:
    pass

best_thresholds = train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)
print("ğŸ‰ è®­ç»ƒå®Œæˆ!")


import time
def generate_submission(model, test_loader, device, test_ids, output_path, thresholds=None):
    """
    ç”ŸæˆKaggleæäº¤æ–‡ä»¶
    """
    model.eval()
    predictions = []  # list of (batch_size, num_labels) numpy arrays

    print("ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    with torch.no_grad():
        start_time = time.time()
        test_loader_tqdm = tqdm(test_loader, bar_format=" {n_fmt}/{total_fmt} {postfix}", mininterval=1.0)

        for i, batch in enumerate(test_loader_tqdm):
            try:
                input_ids, attention_mask = move_batch_to_device(batch, device, has_labels=False)

                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                with torch.cuda.amp.autocast():
                    logits = model(input_ids, attention_mask)

                probs = torch.sigmoid(logits).cpu().numpy()  # (B, C)
                predictions.append(probs)
                cost = time.time() - start_time
                test_loader_tqdm.set_postfix({"Cost": f"{cost:.2f}s"})
            except Exception as e:
                print(f"âŒ åœ¨ batch {i} æŠ¥é”™ï¼š{repr(e)}")
                print(f"batch å†…å®¹ä¿¡æ¯:")
                for j, item in enumerate(batch):
                    if torch.is_tensor(item):
                        print(f"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}")
                    else:
                        print(f"  étensor[{j}]: {type(item)}")
                raise e

    # æ‹¼æ¥é¢„æµ‹ä¸º (N, C)
    if len(predictions) == 0:
        raise RuntimeError("æœªç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœï¼Œpredictions ä¸ºç©º")
    preds_array = np.vstack(predictions).astype(float)  # (N, C)

    # å¯¹é½é•¿åº¦ï¼Œé˜²æ­¢é•¿åº¦ä¸ä¸€è‡´å¯¼è‡´ç©ºå€¼
    n = min(len(test_ids), preds_array.shape[0])
    if n != len(test_ids) or n != preds_array.shape[0]:
        print(f"âš ï¸  é¢„æµ‹è¡Œæ•°({preds_array.shape[0]})ä¸test_ids({len(test_ids)})ä¸ä¸€è‡´ï¼Œå°†æŒ‰è¾ƒå°é•¿åº¦{n}å¯¹é½")
    test_ids = list(test_ids)[:n]
    preds_array = preds_array[:n]

    # åº”ç”¨é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰ï¼Œå¹¶ä¿è¯ä¸ºfloat
    if thresholds is not None:
        thr = np.array(thresholds, dtype=float).reshape(1, -1)  # (1, C)
        preds_array = (preds_array >= thr).astype(float)

    # æ¸…ç†æ•°å€¼ä¸­çš„NaN/Infï¼Œé¿å…ç©ºå€¼å†™å…¥CSV
    preds_array = np.nan_to_num(preds_array, nan=0.0, posinf=1.0, neginf=0.0)

    # åˆ›å»ºæäº¤DataFrame
    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    data_dict = {'id': test_ids}
    for i, col in enumerate(label_columns):
        data_dict[col] = preds_array[:, i].astype(float)
    submission_df = pd.DataFrame(data_dict)

    # ä¿å­˜æäº¤æ–‡ä»¶
    submission_df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    for i, col in enumerate(label_columns):
        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)
        print(f"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}")

    return submission_df

# ç”Ÿæˆæäº¤æ–‡ä»¶
submission_path = os.path.join(data_dir, 'submission.csv')
submission_df = generate_submission(net, test_iter, device, test_ids, submission_path, thresholds=None)
print(f"âœ… æäº¤æ–‡ä»¶: {submission_path}")
##å±è”½è¿›åº¦æ¡ï¼Œgithubä¸­ä¸æ”¯æŒæ˜¾ç¤ºï¼Œæ•´ä¸ªnotebookéƒ½ä¸æ˜¾ç¤ºäº†
import os
# è®¾ç½®è¿™ä¸ªç¯å¢ƒå˜é‡æ¥ç¦ç”¨tqdmè¿›åº¦æ¡
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

import datasets
datasets.disable_progress_bar()

import os
import urllib.request
from pathlib import Path

# === 1. å…¨å±€é…ç½® ===
URLPrefix = "https://pro-5gu0t2os8cdd45f2-1251420592.tcloudbaseapp.com/toxic-comment-classification"
data_dir = 'toxic-comment'
DATA_DIR = Path(data_dir)
FILENAMES = ["train.csv","test.csv","test_labels.csv","sample_submission.csv"]

BATCH_SIZE = 8
RANDOM_STATE = 123
NUM_WORKERS = 2


# === 2. æ•°æ®å‡†å¤‡ ===
def prepare_csv_list():
    # å¦‚æœtoxic-comment ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¯¥ç›®å½•
    if not DATA_DIR.exists():
        DATA_DIR.mkdir(parents=True, exist_ok=True)

    for fileName in FILENAMES:
        URL = f"{URLPrefix}/{fileName}"
        DATA_FILE =DATA_DIR/fileName
        if not DATA_FILE.exists():
            print(f"â¬‡ï¸ Downloading {fileName}...")
            with urllib.request.urlopen(URL) as r, open(DATA_FILE, "wb") as f:
                f.write(r.read())
        else:
            print(f"âœ… already exists: {fileName} ")

import re
import pandas as pd
from transformers import GPT2Tokenizer
import torch
from torch.utils.data import Dataset


class TextPreprocessor:
    """ä½¿ç”¨ GPT2Tokenizer çš„æ–‡æœ¬é¢„å¤„ç†å™¨"""

    def __init__(self,
                 model_name="gpt2",
                 max_seq_length=128,
                 add_special_tokens=True,
                 padding=True,
                 truncation=True):
        """
        Args:
            model_name (str): ä½¿ç”¨çš„é¢„è®­ç»ƒ GPT2 åˆ†è¯å™¨åç§°ï¼ˆå¦‚ "gpt2", "gpt2-medium"ï¼‰
            max_seq_length (int): æœ€å¤§åºåˆ—é•¿åº¦
            add_special_tokens (bool): æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ BOS/EOSï¼‰
            padding (bool): æ˜¯å¦è‡ªåŠ¨å¡«å……
            truncation (bool): æ˜¯å¦æˆªæ–­è¶…é•¿æ–‡æœ¬
        """
        self.max_seq_length = max_seq_length
        self.add_special_tokens = add_special_tokens
        self.padding = padding
        self.truncation = truncation

        # âœ… åˆå§‹åŒ– GPT2 åˆ†è¯å™¨
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        # GPT-2 é»˜è®¤æ²¡æœ‰ pad_tokenï¼Œéœ€è¦æ‰‹åŠ¨è®¾ç½®
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print(f"âœ… GPT2Tokenizer å·²åŠ è½½: {model_name}")
        print(f"è¯è¡¨å¤§å°: {len(self.tokenizer)}")

    def clean_text(self, text: str) -> str:
        """å¯é€‰çš„æ–‡æœ¬æ¸…ç†ï¼ˆä¿ç•™åŸºç¡€æ¸…æ´—é€»è¾‘ï¼‰"""
        if pd.isna(text):
            return ""
        text = str(text).strip()
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_vocab(self, texts):
        """å…¼å®¹æ—§æ¥å£ï¼ˆGPT2Tokenizer è‡ªå¸¦è¯æ±‡è¡¨ï¼Œæ— éœ€æ‰‹åŠ¨æ„å»ºï¼‰"""
        print("âš™ï¸ ä½¿ç”¨ GPT2Tokenizer è‡ªå¸¦è¯æ±‡è¡¨ï¼Œæ— éœ€æ‰‹åŠ¨æ„å»ºã€‚")
        return list(self.tokenizer.get_vocab().keys())

    def text_to_sequence(self, text):
        """æ–‡æœ¬è½¬ä¸º GPT-2 Token ID åºåˆ—"""
        cleaned_text = self.clean_text(text)
        encoding = self.tokenizer(
            cleaned_text,
            add_special_tokens=self.add_special_tokens,
            max_length=self.max_seq_length,
            padding='max_length' if self.padding else False,
            truncation=self.truncation,
            return_tensors=None
        )
        return encoding["input_ids"]

    def batch_encode(self, texts):
        """æ‰¹é‡æ–‡æœ¬ç¼–ç """
        cleaned_texts = [self.clean_text(t) for t in texts]
        encodings = self.tokenizer(
            cleaned_texts,
            add_special_tokens=self.add_special_tokens,
            max_length=self.max_seq_length,
            padding='max_length' if self.padding else False,
            truncation=self.truncation,
            return_tensors=None
        )
        return encodings["input_ids"]


class ToxicCommentDataset(Dataset):
    """æœ‰æ¯’è¯„è®ºæ•°æ®é›†"""

    def __init__(self, texts, labels, preprocessor):
        self.texts = texts
        # self.labels = labels if labels is not None else [[0]*6]*len(texts)
        self.labels = labels if labels is not None else [[0]*6 for _ in range(len(texts))]

        self.preprocessor = preprocessor

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        sequence = self.preprocessor.text_to_sequence(text)

        if not sequence or len(sequence) == 0:
            sequence = [self.preprocessor.tokenizer.pad_token_id] * self.preprocessor.max_seq_length


        # åˆ›å»ºattention maskï¼ˆéé›¶ä½ç½®ä¸º1ï¼‰
        attention_mask = [1 if token != 0 else 0 for token in sequence]

        return (
            torch.tensor(sequence, dtype=torch.long),
            torch.tensor(attention_mask, dtype=torch.long),
            torch.tensor(self.labels[idx], dtype=torch.float)
        )


def read_toxic_comments_real(data_dir, max_samples=None, is_train=True):
    """
    è¯»å–çœŸå®çš„Kaggle Toxic Comment Classificationæ•°æ®
    è¿”å›æ ¼å¼: (texts, labels, ids)
    """
    if is_train:
        csv_path = os.path.join(data_dir, 'train.csv')
        print(f"è¯»å–è®­ç»ƒæ•°æ®: {csv_path}")

        df = pd.read_csv(csv_path)
        if max_samples:
            df = df.head(max_samples)

        texts = df['comment_text'].tolist()
        label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
        labels = df[label_columns].values.tolist()
        ids = df['id'].tolist()

        print(f"åŠ è½½è®­ç»ƒæ•°æ®: {len(texts)} æ¡")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(label_columns, df[label_columns].sum().tolist()))}")

        return texts, labels, ids
    else:
        csv_path = os.path.join(data_dir, 'test.csv')
        print(f"è¯»å–æµ‹è¯•æ•°æ®: {csv_path}")

        df = pd.read_csv(csv_path)
        if max_samples:
            df = df.head(max_samples)

        texts = df['comment_text'].tolist()
        ids = df['id'].tolist()

        print(f"åŠ è½½æµ‹è¯•æ•°æ®: {len(texts)} æ¡")

        return texts, None, ids

def create_dataloaders(data_dir,batch_size):
# æ•°æ®ç›®å½•
    prepare_csv_list()

    # æ•°æ®åŠ è½½
    print("ğŸ“Š åŠ è½½çœŸå®Kaggleæ•°æ®...")

    # ä¸ºäº†å¿«é€Ÿè®­ç»ƒï¼Œé™åˆ¶æ ·æœ¬æ•°ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    train_texts, train_labels, train_ids = read_toxic_comments_real(
        data_dir, max_samples=None, is_train=True
    )

    # åˆ›å»ºéªŒè¯é›†ï¼ˆä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²ï¼‰
    val_split = int(len(train_texts) * 0.8)
    val_texts = train_texts[val_split:]
    val_labels = train_labels[val_split:]
    train_texts = train_texts[:val_split]
    train_labels = train_labels[:val_split]

    # è¯»å–æµ‹è¯•æ•°æ®
    test_texts, _, test_ids = read_toxic_comments_real(
        data_dir, max_samples=None, is_train=False
    )

    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"è®­ç»ƒæ•°æ®: {len(train_texts)} æ¡")
    print(f"éªŒè¯æ•°æ®: {len(val_texts)} æ¡")
    print(f"æµ‹è¯•æ•°æ®: {len(test_texts)} æ¡")

    # æ£€æŸ¥æ•°æ®è´¨é‡
    print(f"\nğŸ“ æ•°æ®æ ·ä¾‹:")
    print(f"æ–‡æœ¬é•¿åº¦: {len(train_texts[0])}")
    print(f"å‰100å­—ç¬¦: {train_texts[0][:100]}")
    print(f"æ ‡ç­¾: {train_labels[0]}")

    preprocessor = TextPreprocessor()

    print(f"\nğŸ”§ é¢„å¤„ç†å™¨æµ‹è¯•:")
    sample_sequence = preprocessor.text_to_sequence(train_texts[0])
    print(f"åºåˆ—é•¿åº¦: {len(sample_sequence)}")
    print(f"éé›¶tokenæ•°: {sum(1 for x in sample_sequence if x != 0)}")



    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = ToxicCommentDataset(train_texts, train_labels, preprocessor)
    val_dataset = ToxicCommentDataset(val_texts, val_labels, preprocessor)
    test_dataset = ToxicCommentDataset(test_texts, None, preprocessor)

    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                              num_workers=4, pin_memory=True, persistent_workers=True)
    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                                           num_workers=2, pin_memory=True, persistent_workers=True)
    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                                            num_workers=2, pin_memory=True, persistent_workers=True)
    return train_iter, val_iter, test_iter,test_ids


    #!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kaggle Toxic Comment Classification - gpt2 ç‰ˆæœ¬ å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
from transformers import GPT2Tokenizer, GPT2Model
from collections import Counter
import re
import os
from tqdm import tqdm
import time
import warnings
warnings.filterwarnings('ignore')

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"NumPy: {np.__version__}")


class GPT2ClassificationModel(nn.Module):
  def __init__(self,num_labels = 6):
    super().__init__()
    self.gpt2 = GPT2Model.from_pretrained('gpt2')
    config = self.gpt2.config
    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)

  def forward(self,input_ids,attention_mask):
    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)
    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])
    return logits

# ====== è§£å†»æ§åˆ¶ï¼šåªè§£å†»é¡¶å±‚è‹¥å¹²å±‚ ======
def unfreeze_gpt2_top_k_layers(model: GPT2ClassificationModel, k: int) -> None:
    """å†»ç»“ GPT-2 å…¨éƒ¨å±‚åï¼Œä»…è§£å†»é¡¶å±‚ k ä¸ª blockï¼ˆä»¥åŠæœ€ç»ˆå±‚å½’ä¸€åŒ–ä¸åˆ†ç±»å¤´ï¼‰ã€‚

    Args:
        model: åŒ…å« GPT-2 çš„åˆ†ç±»æ¨¡å‹
        k: éœ€è¦è§£å†»çš„é¡¶å±‚ block æ•°é‡ï¼ˆk<=æ€»å±‚æ•°ï¼‰ã€‚k<=0 è¡¨ç¤ºåªè®­ç»ƒåˆ†ç±»å¤´
    """
    # å†»ç»“å…¨éƒ¨ GPT-2 å‚æ•°
    for p in model.gpt2.parameters():
        p.requires_grad = False

    # ä»…è§£å†»é¡¶å±‚ k ä¸ª block
    if k > 0:
        total_blocks = len(model.gpt2.h)
        k = min(k, total_blocks)
        for block in model.gpt2.h[-k:]:
            for p in block.parameters():
                p.requires_grad = True
        # åŒæ—¶è§£å†»æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºé€‚é…ä¸‹æ¸¸ä»»åŠ¡
        for p in model.gpt2.ln_f.parameters():
            p.requires_grad = True

    # å§‹ç»ˆè®­ç»ƒåˆ†ç±»å¤´
    for p in model.classifier.parameters():
        p.requires_grad = True

# å·¥å…·ç±»
class Accumulator:
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]

class Timer:
    def __init__(self):
        import time
        self.time = time
        self.start_time = self.time.time()
    def stop(self):
        return self.time.time() - self.start_time

def try_all_gpus():
    """æ£€æµ‹å¯ç”¨GPU"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')



def multilabel_accuracy(y_hat, y):
    """å¤šæ ‡ç­¾åˆ†ç±»å‡†ç¡®ç‡"""
    predictions = torch.sigmoid(y_hat) > 0.5
    y = y.bool()
    label_wise_acc = (predictions == y).float().mean()
    return label_wise_acc.item()

def train_gpt2_epoch(net, train_iter, loss, updater, device, scheduler=None,progress_bar=None):
    """
    å•ä¸ªepochè®­ç»ƒ - æ··åˆç²¾åº¦è®­ç»ƒ + å­¦ä¹ ç‡è°ƒåº¦
    """
    net.train()
    metric = Accumulator(3)  # è®­ç»ƒæŸå¤±æ€»å’Œ, å‡†ç¡®æ•°, æ ·æœ¬æ•°

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None

    start_time = time.time()
    for _, batch in enumerate(train_iter):
        input_ids, attention_mask, labels = batch
        input_ids = input_ids.to(device, non_blocking=True)
        attention_mask = attention_mask.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        if scaler is not None:
            with torch.cuda.amp.autocast():
                y_hat = net(input_ids, attention_mask)
                l = loss(y_hat, labels)
        else:
            y_hat = net(input_ids, attention_mask)
            l = loss(y_hat, labels)

        updater.zero_grad()

        # æ··åˆç²¾åº¦åå‘ä¼ æ’­
        if scaler is not None:
            scaler.scale(l.sum()).backward()
            scaler.unscale_(updater)
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
            scaler.step(updater)
            scaler.update()
        else:
            l.sum().backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
            updater.step()

        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRéœ€è¦åœ¨æ¯ä¸ªbatchåè°ƒç”¨ï¼‰
        if scheduler is not None:
            scheduler.step()

        with torch.no_grad():
            acc = multilabel_accuracy(y_hat, labels)
            metric.add(l.sum(), acc * labels.shape[0], labels.shape[0])

        cost = time.time() - start_time
        if progress_bar is not None:
            progress_bar.set_postfix({"Cost": f"{cost:.2f}s"})

    return metric[0] / metric[2], metric[1] / metric[2]

def evaluate_gpt2_accuracy(net, data_iter, device):
    net.eval()
    metric = Accumulator(2)
    with torch.no_grad():
        for batch in data_iter:
            input_ids, attention_mask, labels = batch
            input_ids = input_ids.to(device, non_blocking=True)
            attention_mask = attention_mask.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
            if device.type == 'cuda':
                with torch.cuda.amp.autocast():
                    y_hat = net(input_ids, attention_mask)
            else:
                y_hat = net(input_ids, attention_mask)

            acc = multilabel_accuracy(y_hat, labels)
            metric.add(acc * labels.shape[0], labels.shape[0])
    return metric[0] / metric[1]

def train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, devices, scheduler=None):
    """
    å®Œæ•´è®­ç»ƒæµç¨‹
    """
    print('training on', devices)

    if isinstance(devices, list) and len(devices) > 1:
        # å¤šGPU
        net = nn.DataParallel(net, device_ids=devices)

    device = devices[0] if isinstance(devices, list) else devices
    net = net.to(device)

    for epoch in range(num_epochs):
        train_iter_tqdm = tqdm(train_iter,
                            desc=f"Epoch {epoch+1}/{num_epochs}",
                            bar_format="{desc}: {n_fmt}/{total_fmt} {postfix}")

        # è®­ç»ƒ
        train_loss, train_acc = train_gpt2_epoch(
            net, train_iter_tqdm, loss, trainer, device, scheduler,train_iter_tqdm
        )

        # éªŒè¯
        val_acc = evaluate_gpt2_accuracy(net, val_iter, device)

        tqdm.write(f'Epoch {epoch + 1}: '
              f'loss {train_loss:.3f}, '
              f'train acc {train_acc:.3f}, '
              f'val acc {val_acc:.3f}, '
              f'lr {trainer.param_groups[0]["lr"]:.6f}')

    # print(f'Training completed in {timer.stop():.1f} sec')
    print(f'Final: train acc {train_acc:.3f}, val acc {val_acc:.3f}')


# ============ ä¸»è¦æ‰§è¡Œä»£ç  ============
print("ğŸš€ å¯åŠ¨åŒå‘GPT2å¤šæ ‡ç­¾åˆ†ç±»è®­ç»ƒ")

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}')

# æ¨¡å‹å‚æ•° - ä¼˜åŒ–ä»¥æå‡è®­ç»ƒé€Ÿåº¦
num_classes = 6  # 6ä¸ªç±»åˆ«ï¼štoxic, severe_toxic, obscene, threat, insult, identity_hate
# æ ¹æ®GPUæƒ…å†µè‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
batch_size = 32 if torch.cuda.is_available() else 16
num_steps = 128   # åºåˆ—é•¿åº¦ï¼ˆä»128é™åˆ°64ï¼‰
lr = 2e-3        # æé«˜å­¦ä¹ ç‡ä»¥åŠ å¿«æ”¶æ•›
num_epochs = 3   # è®­ç»ƒè½®æ•°

train_iter, val_iter, test_iter, test_ids =  create_dataloaders(data_dir,batch_size)

net = GPT2ClassificationModel()
print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
net.to(device)

# --- è§£å†»ç­–ç•¥ï¼šåªè§£å†»é¡¶å±‚è‹¥å¹²å±‚ï¼ˆå…¶ä½™å†»ç»“ï¼‰ ---
UNFREEZE_TOP_K = 2  # ä¿®æ”¹æ­¤å€¼æ§åˆ¶è§£å†»çš„é¡¶å±‚ block æ•°é‡ï¼›0 è¡¨ç¤ºä»…è®­ç»ƒåˆ†ç±»å¤´
unfreeze_gpt2_top_k_layers(net, k=UNFREEZE_TOP_K)
trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)
print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,} (è§£å†»é¡¶å±‚ {UNFREEZE_TOP_K} å±‚)")
# ------------------------------------


# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰
trainer = optim.Adam((p for p in net.parameters() if p.requires_grad), lr=lr, weight_decay=0.01)

# æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ä»¥æå‡è®­ç»ƒæ•ˆæœ
scheduler = optim.lr_scheduler.OneCycleLR(
    trainer,
    max_lr=lr * 5,  # æœ€å¤§å­¦ä¹ ç‡
    steps_per_epoch=len(train_iter),
    epochs=num_epochs,
    pct_start=0.3  # å‰30%æ—¶é—´ç”¨äºå‡æ¸©
)

# æŸå¤±å‡½æ•° - å¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨BCEWithLogitsLoss
loss = nn.BCEWithLogitsLoss(reduction="none")  # æ¯ä¸ªæ ·æœ¬æ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹è®¡ç®—


# å¼€å§‹è®­ç»ƒ - ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
# train_gpt2_model(net, train_iter, val_iter, loss, trainer, num_epochs, device, scheduler)

print("\n" + "="*60)
print("ğŸ‰ Huggingface GPT2 å¤šæ ‡ç­¾åˆ†ç±» è®­ç»ƒå®Œæˆ!")


import time
from tqdm.std import tqdm
def generate_submission(model, test_loader, device, test_ids, output_path):
    """
    ç”ŸæˆKaggleæäº¤æ–‡ä»¶
    """
    model.eval()
    predictions = []

    print("ğŸ”® ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    with torch.no_grad():
        start_time = time.time()
        test_loader_tqdm = tqdm(test_loader,bar_format=" {n_fmt}/{total_fmt} {postfix}")

        for batch in test_loader_tqdm:
            try:
                input_ids, attention_mask, _ = batch
                input_ids = input_ids.to(device, non_blocking=True)
                attention_mask = attention_mask.to(device, non_blocking=True)

                # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                if device.type == 'cuda':
                    with torch.cuda.amp.autocast():
                        logits = model(input_ids, attention_mask)
                else:
                    logits = model(input_ids, attention_mask)

                probs = torch.sigmoid(logits).cpu().numpy()
                predictions.extend(probs)
                cost = time.time() - start_time
                test_loader_tqdm.set_postfix({"Cost": f"{cost:.2f}s"})
            except Exception as e:
                print(f"âŒ åœ¨ batch {i} æŠ¥é”™ï¼š{repr(e)}")
                print(f"batch å†…å®¹ä¿¡æ¯:")
                for j, item in enumerate(batch):
                    if torch.is_tensor(item):
                        print(f"  tensor[{j}] -> shape: {item.shape}, dtype: {item.dtype}")
                    else:
                        print(f"  étensor[{j}]: {type(item)}")
                raise e


    # åˆ›å»ºæäº¤DataFrame
    label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    submission_df = pd.DataFrame({
        'id': test_ids,
        **{col: [pred[i] for pred in predictions] for i, col in enumerate(label_columns)}
    })

    # ä¿å­˜æäº¤æ–‡ä»¶
    submission_df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    for i, col in enumerate(label_columns):
        avg_prob = sum(pred[i] for pred in predictions) / len(predictions)
        print(f"  {col}: å¹³å‡æ¦‚ç‡ {avg_prob:.4f}")

    return submission_df

    # ç”Ÿæˆæäº¤æ–‡ä»¶
submission_path = os.path.join(data_dir, 'submission.csv')
submission_df = generate_submission(net, test_iter, device, test_ids, submission_path)

print("\nğŸ‰ è®­ç»ƒå’Œé¢„æµ‹å®Œæˆ!")
print(f"âœ… æäº¤æ–‡ä»¶: {submission_path}")
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/finetuning-for-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ËøõË°åÊñáÊú¨ÂàÜÁ±ªÁöÑÂæÆË∞É"
      ],
      "metadata": {
        "id": "JLfSRs2smELT"
      },
      "id": "JLfSRs2smELT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_any\"\n",
        "login(token=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "#Ê†°È™åÊòØÂê¶ËÆæÁΩÆÊàêÂäü\n",
        "! echo $HF_TOKEN\n",
        "```"
      ],
      "metadata": {
        "id": "ZFHQ1SfVpWjN"
      },
      "id": "ZFHQ1SfVpWjN"
    },
    {
      "cell_type": "code",
      "source": [
        "##Â±èËîΩËøõÂ∫¶Êù°Ôºågithub‰∏≠‰∏çÊîØÊåÅÊòæÁ§∫ÔºåÊï¥‰∏™notebookÈÉΩ‰∏çÊòæÁ§∫‰∫Ü\n",
        "import os\n",
        "# ËÆæÁΩÆËøô‰∏™ÁéØÂ¢ÉÂèòÈáèÊù•Á¶ÅÁî®tqdmËøõÂ∫¶Êù°\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        " # ÂÆö‰πâ‰∏Ä‰∏™‰∏çÂåÖÂê´ {bar} Âç†‰ΩçÁ¨¶ÁöÑËá™ÂÆö‰πâÊ†ºÂºèÂ≠óÁ¨¶‰∏≤\n",
        "# {desc} - ÊèèËø∞ÊñáÊú¨\n",
        "# {percentage:3.0f}% - ÁôæÂàÜÊØî\n",
        "# {n_fmt}/{total_fmt} - ÂΩìÂâç/ÊÄªÊï∞\n",
        "# {elapsed}<{remaining} - Â∑≤ËøáÊó∂Èó¥ < Ââ©‰ΩôÊó∂Èó¥\n",
        "# {rate_fmt} - ÈÄüÁéá\n",
        "custom_format = \"{desc}: {percentage:3.0f}% | {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\""
      ],
      "metadata": {
        "id": "7LU4K6Ceqde6"
      },
      "id": "7LU4K6Ceqde6",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface gpt2\n",
        "‰ΩøÁî®[huggingface gpt2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "‰∏∫‰∫ÜÁêÜËß£ huggingface gpt2ÁöÑ‰ΩøÁî®ÔºåÂÜô‰∏Ä‰∏™ÊñáÊú¨ÁîüÊàêÁ§∫‰æã‰ª£Á†ÅÔºåÊÑüÂèó‰∏ã"
      ],
      "metadata": {
        "id": "xLoXMx-zmUCJ"
      },
      "id": "xLoXMx-zmUCJ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f4855611",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4855611",
        "outputId": "0395da62-da80-4e52-9b38-7fe423924d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "**********\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(model.config)\n",
        "print('*'*10)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(model.wte.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ëá™ÂÆöÁîüÊàêÊñáÊú¨Model"
      ],
      "metadata": {
        "id": "dC1hF-UgnGxI"
      },
      "id": "dC1hF-UgnGxI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class MyGPT2LMHeadModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.gpt2.wte.weight\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    gpt2_out = self.gpt2(in_idx)\n",
        "    logits = self.lm_head(gpt2_out.last_hidden_state)\n",
        "    return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDvbZlhfbWDM"
      },
      "id": "YDvbZlhfbWDM",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning,\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‰ΩøÁî®ÁöÑËÆæÂ§á: {device}\")\n",
        "myGPT2LMHeadModel = MyGPT2LMHeadModel()\n",
        "myGPT2LMHeadModel.to(device)\n",
        "token_ids = generate(\n",
        "    model=myGPT2LMHeadModel,\n",
        "    idx= tokenizer.encode(prompt_text, return_tensors='pt').to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=1024,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", tokenizer.decode(token_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHn7gz10x4a",
        "outputId": "79e8f66f-3148-41b9-fa6c-be55d3f854c6"
      },
      "id": "RYHn7gz10x4a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‰ΩøÁî®ÁöÑËÆæÂ§á: cuda\n",
            "Output text:\n",
            " In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, as Whiskers was asleep, the pet arrived at her door, where she took the child to the local hospital for check-up. It was a short time later, as Whiskers was back in her crib and on the side of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‰ΩøÁî®transformersÁöÑGPT2LMHeadModelÁîüÊàêÊñáÊú¨"
      ],
      "metadata": {
        "id": "F4YtWaXDnLGk"
      },
      "id": "F4YtWaXDnLGk"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1203d226",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1203d226",
        "outputId": "fc8f8bed-e4a8-4bde-d535-53cd2289c575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‰ΩøÁî®ÁöÑËÆæÂ§á: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ê≠£Âú®ÁîüÊàêÊïÖ‰∫ã...\n",
            "\n",
            "--- ÁîüÊàêÁöÑÊïÖ‰∫ã ---\n",
            "In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, he'd seen a couple of her adorable little kittens, then a night owl and then his sister. The cat had always been the most popular.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Ê£ÄÊü•ÊòØÂê¶ÊúâÂèØÁî®ÁöÑ GPUÔºåÂπ∂ËÆæÁΩÆËÆæÂ§á\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‰ΩøÁî®ÁöÑËÆæÂ§á: {device}\")\n",
        "\n",
        "# 1. Âä†ËΩΩÂ∏¶ÊúâËØ≠Ë®ÄÊ®°ÂûãÂ§¥ÁöÑÊ®°ÂûãÂíåÂàÜËØçÂô®\n",
        "# Â∞ÜÊ®°ÂûãÁßªÂä®Âà∞ÊåáÂÆöÁöÑËÆæÂ§á‰∏ä\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "modelLMHead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "modelLMHead.to(device)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# 4. ‰ΩøÁî® model.generate() ÁîüÊàêÊñáÊú¨\n",
        "print(\"\\nÊ≠£Âú®ÁîüÊàêÊïÖ‰∫ã...\")\n",
        "# Ë∞ÉÁî® generate ÊñπÊ≥ïÊù•Âàõ‰ΩúÊïÖ‰∫ã\n",
        "output_sequences = modelLMHead.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=50,          # ÁîüÊàêÊñáÊú¨ÁöÑÊúÄÂ§ßÈïøÂ∫¶ÔºàÂåÖÂê´ÊèêÁ§∫Ôºâ\n",
        "    num_return_sequences=1,  # ÁîüÊàêÂá†‰∏™‰∏çÂêåÁöÑÊïÖ‰∫ã\n",
        "    no_repeat_ngram_size=2,  # ÈÅøÂÖçÈáçÂ§çÁü≠ËØ≠ÁöÑÂÖ≥ÈîÆÂèÇÊï∞\n",
        "    do_sample=True,          # ÂêØÁî®ÈááÊ†∑ÔºåËÆ©ÊñáÊú¨Êõ¥ÊúâÂàõÊÑèÔºåËÄå‰∏çÊòØÊ≠ªÊùøÁöÑÈ¢ÑÊµã\n",
        "    temperature=0.9,         # ÊéßÂà∂ÂàõÈÄ†ÊÄß‰∏éÁ°ÆÂÆöÊÄßÁöÑÂπ≥Ë°°ÔºåÊï∞ÂÄºË∂ä‰ΩéË∂ä‰øùÂÆà\n",
        "    top_k=50,                # ÈááÊ†∑Êó∂Âè™ËÄÉËôëÊ¶ÇÁéáÊúÄÈ´òÁöÑ50‰∏™ËØç\n",
        "    top_p=0.95,              # Ê†∏ÂøÉÈááÊ†∑Ôºå‰øùÁïôÊ¶ÇÁéáÊÄªÂíå‰∏∫95%ÁöÑËØçÊ±á\n",
        ")\n",
        "\n",
        "# 5. Ëß£Á†ÅÁîüÊàêÁöÑÊñáÊú¨\n",
        "# Â∞ÜÁîüÊàêÁöÑÊï∞Â≠óIDÂ∫èÂàóËΩ¨Êç¢Âõû‰∫∫Á±ªÂèØËØªÁöÑÂ≠óÁ¨¶‰∏≤\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. ÊâìÂç∞ÁªìÊûú\n",
        "print(\"\\n--- ÁîüÊàêÁöÑÊïÖ‰∫ã ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuning-for-classification"
      ],
      "metadata": {
        "id": "2pgzANIXoXIO"
      },
      "id": "2pgzANIXoXIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Â¶ÇÊûúÊÇ®ÂÖ∑ÊúâÊú∫Âô®Â≠¶‰π†ÁöÑËÉåÊôØÔºåÂØπ‰∫éÂàÜÁ±ªÂæÆË∞ÉÊÇ®ÂèØËÉΩÂ∑≤ÁªèÁÜüÊÇâ. ‰∏æ‰∏™‰æãÂ≠êÔºåÂàÜÁ±ªÂæÆË∞ÉÁ±ª‰ºº‰∫éËÆ≠ÁªÉÂç∑ÁßØÁΩëÁªúÊù•ÂØπÊâãÂÜôÊï∞Â≠óËøõË°åÂàÜÁ±ªÁöÑËøáÁ®ã\n",
        "- Âú®ÂàÜÁ±ªÂæÆË∞É‰∏≠ÔºåÊ®°ÂûãÂèØ‰ª•ËæìÂá∫ÁâπÂÆöÁöÑÂàÜÁ±ªÊ†áÁ≠æÔºà‰æãÂ¶ÇÔºå‚Äúspam‚ÄùÂíå‚Äúnot spam‚ÄùÔºâ\n",
        "- ÂàÜÁ±ªÂæÆË∞ÉÊ®°ÂûãÂè™ËÉΩÈ¢ÑÊµãÂÆÉÂú®ËÆ≠ÁªÉÊúüÈó¥ÊâÄÁÜüÁü•ÁöÑÁ±ªÂà´Ê†áÁ≠æÔºà‰æãÂ¶ÇÔºå‚ÄúÂûÉÂúæÈÇÆ‰ª∂‚ÄùÊàñ‚ÄúÈùûÂûÉÂúæÈÇÆ‰ª∂‚ÄùÔºâÔºåËÄåÊåá‰ª§ÂæÆË∞ÉÊ®°ÂûãÈÄöÂ∏∏ÂèØ‰ª•ÊâßË°åÊõ¥ÂπøÊ≥õÁöÑ‰ªªÂä°\n",
        "- Êàë‰ª¨ÂèØ‰ª•Â∞ÜÂàÜÁ±ªÂæÆË∞ÉÊ®°ÂûãËßÜ‰∏∫È´òÂ∫¶‰∏ì‰∏öÂåñÁöÑÊ®°Âûã;Âú®ÂÆûË∑µ‰∏≠ÔºåÂºÄÂèë‰∏ì‰∏öÂåñÁöÑÊ®°ÂûãÈÄöÂ∏∏ÊØîÂºÄÂèëÂú®ËÆ∏Â§ö‰∏çÂêå‰ªªÂä°‰∏äË°®Áé∞ËâØÂ•ΩÁöÑÈÄöÁî®Ê®°ÂûãË¶ÅÂÆπÊòìÂæóÂ§ö"
      ],
      "metadata": {
        "id": "uCbfj-l1znk-"
      },
      "id": "uCbfj-l1znk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ÂáÜÂ§áÊï∞ÊçÆÈõÜ\n",
        "Êàë‰ª¨‰ΩøÁî®Áî±ÂûÉÂúæÈÇÆ‰ª∂ÂíåÈùûÂûÉÂúæÈÇÆ‰ª∂ÁªÑÊàêÁöÑÊï∞ÊçÆÈõÜÊù•ÂØπ LLM ËøõË°åÂàÜÁ±ªÂæÆË∞É"
      ],
      "metadata": {
        "id": "MT8VVyPtzuLT"
      },
      "id": "MT8VVyPtzuLT"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# === 1. ÂÖ®Â±ÄÈÖçÁΩÆ ===\n",
        "URL = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "ZIP_PATH = \"sms_spam_collection.zip\"\n",
        "DATA_DIR = Path(\"sms_spam_collection\")\n",
        "DATA_FILE = DATA_DIR / \"SMSSpamCollection.tsv\"\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. Êï∞ÊçÆÂáÜÂ§á ===\n",
        "def prepare_data():\n",
        "    if not DATA_FILE.exists():\n",
        "        print(\"‚¨áÔ∏è Downloading and extracting dataset...\")\n",
        "        with urllib.request.urlopen(URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        os.rename(DATA_DIR / \"SMSSpamCollection\", DATA_FILE)\n",
        "    else:\n",
        "        print(\"‚úÖ Dataset already exists.\")\n",
        "\n",
        "    df = pd.read_csv(DATA_FILE, sep=\"\\t\", names=[\"Label\", \"Text\"])\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(df[\"Label\"].value_counts())\n",
        "\n",
        "    # Âπ≥Ë°°Êï∞ÊçÆÈõÜÔºåham:4825 ,spam:747Ôºå ‰ΩøÊØè‰∏™Á±ªÂà´ÂåÖÂê´ 747 ‰∏™ÂÆû‰æã„ÄÇ\n",
        "    ham = df[df[\"Label\"] == \"ham\"].sample(n=df.Label.value_counts()[\"spam\"], random_state=RANDOM_STATE)\n",
        "    df = pd.concat([ham, df[df[\"Label\"] == \"spam\"]])\n",
        "    df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    return df[:int(0.7*n)], df[int(0.7*n):int(0.8*n)], df[int(0.8*n):]\n",
        "\n",
        "\n",
        "# === 3. Tokenizer & Dataset ‰ΩøÁî® Hugging Face `datasets` Â∫ì ===\n",
        "def build_datasetDict(train_df, val_df, test_df):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    datasetDict = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"validation\": Dataset.from_pandas(val_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    print(\"\\nüî¢ Calculating max sequence length (may take a few seconds)...\")\n",
        "    train_texts = datasetDict[\"train\"][\"Text\"]\n",
        "\n",
        "\n",
        "    max_len = max(len(tokenizer.encode(t)) for t in tqdm(train_texts,bar_format=custom_format))\n",
        "    print(f\"Max length: {max_len}\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "\n",
        "    print(\"\\n‚úÇÔ∏è Tokenizing datasets...\")\n",
        "    tokenized = datasetDict.map(tokenize_fn, batched=True, remove_columns=[\"Text\"])\n",
        "    tokenized = tokenized.rename_column(\"Label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[ \"input_ids\", \"attention_mask\",\"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# === 4. DataLoader ===\n",
        "def create_loaders(datasetDict):\n",
        "    return {\n",
        "        split: DataLoader(ds, batch_size=BATCH_SIZE,  shuffle=(split == \"train\"),num_workers=NUM_WORKERS)\n",
        "        for split, ds in datasetDict.items()\n",
        "    }\n",
        "\n",
        "\n",
        "# === ‰∏ªÊµÅÁ®ã ===\n",
        "print(\"üèó Preparing data...\")\n",
        "train_df, val_df, test_df = prepare_data()\n",
        "datasetDict = build_datasetDict(train_df, val_df, test_df)\n",
        "loaders = create_loaders(datasetDict)\n",
        "\n",
        "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"validation\"], loaders[\"test\"]\n",
        "\n",
        "print(\"\\n‚úÖ Data pipeline ready!\")\n",
        "for name, loader in loaders.items():\n",
        "    print(f\"{name:>10}: {len(loader)} batches\")\n",
        "\n",
        "# === Á§∫‰æãËæìÂá∫ ===\n",
        "print(\"\\nüì¶ Example batch from train_loader:\")\n",
        "batch = next(iter(loaders[\"train\"]))\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k:>15}:\", v.shape)\n",
        "\n",
        "# Êü•ÁúãÂÆûÈôÖÊñáÊú¨\n",
        "print(\"\\nüìù Decoded sample text:\")\n",
        "sample_ids = batch[\"input_ids\"][0]\n",
        "decoded = GPT2Tokenizer.from_pretrained(\"gpt2\").decode(sample_ids, skip_special_tokens=True)\n",
        "print(decoded)\n",
        "print(\"Label:\", batch[\"labels\"][0].item())\n"
      ],
      "metadata": {
        "id": "ntTbiSLy4sGi",
        "outputId": "ceccf2d0-6de7-430e-f3a3-efe1b36f5e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ntTbiSLy4sGi",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèó Preparing data...\n",
            "‚úÖ Dataset already exists.\n",
            "Loaded 5572 samples\n",
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üî¢ Calculating max sequence length (may take a few seconds)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% | 1045/1045 [00:00<00:00, 2366.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 120\n",
            "\n",
            "‚úÇÔ∏è Tokenizing datasets...\n",
            "\n",
            "‚úÖ Data pipeline ready!\n",
            "     train: 131 batches\n",
            "validation: 19 batches\n",
            "      test: 38 batches\n",
            "\n",
            "üì¶ Example batch from train_loader:\n",
            "         labels: torch.Size([8])\n",
            "      input_ids: torch.Size([8, 120])\n",
            " attention_mask: torch.Size([8, 120])\n",
            "\n",
            "üìù Decoded sample text:\n",
            "Bored of speed dating? Try SPEEDCHAT, txt SPEEDCHAT to 80155, if you don't like em txt SWAP and get a new chatter! Chat80155 POBox36504W45WQ 150p/msg rcd 16\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "KTYb7GO9SWjd"
      },
      "id": "KTYb7GO9SWjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ê®°Âûã‰∏éËÆ≠ÁªÉ"
      ],
      "metadata": {
        "id": "mT-AD-VLUrfI"
      },
      "id": "mT-AD-VLUrfI"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6093a7ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "5e829ff0f1a14f86a50f5f8e644d07d4",
            "00c7c8541fcc4ae3b8b23b81aa121056",
            "5daebed8085149e99ea573708cf42008",
            "5acd5c08ddf94a1fbe63427ed1459c60",
            "7c86ac1f35e84cfba4737a2e2bf353ae",
            "af031ee173e642b3b81fea951659e1e1",
            "23d8919c426a458d870ef66293e70c40",
            "29b8e2a2e7614a368c3b50dd76a79a32",
            "e66ee5d7083e4d899e754492fb89671f",
            "eeb2136943ea4c36877bd2998062933a",
            "7e9849d208c44cf7849868df3e007262"
          ]
        },
        "id": "6093a7ed",
        "outputId": "a9873d32-2795-4c4d-9ff5-74d3833d16a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0% | 0/393 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e829ff0f1a14f86a50f5f8e644d07d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 1/3 ---\n",
            "Validation Loss: 0.0302 | Validation Accuracy: 0.9933\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1873397991.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# 2. ËÆ≠ÁªÉÂæ™ÁéØ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbar_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m train(\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1873397991.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, lr_scheduler, device, progress_bar)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             outputs = model(\n\u001b[1;32m     22\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "def train(model, train_loader,val_loader, optimizer,loss_fn, lr_scheduler, device,progress_bar):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(outputs, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # 3. ËØÑ‰º∞Âæ™ÁéØ\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        total_loss = 0  # ÂàùÂßãÂåñÊÄªÊçüÂ§±\n",
        "\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"])\n",
        "                loss = loss_fn(logits, batch[\"labels\"])\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total_samples += len(batch[\"labels\"])\n",
        "\n",
        "        # ËÆ°ÁÆóÂπ≥ÂùáÊçüÂ§±ÂíåÂáÜÁ°ÆÁéá\n",
        "        avg_val_loss = total_loss / len(val_loader)\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. ÂàùÂßãÂåñÊ®°Âûã„ÄÅ‰ºòÂåñÂô®„ÄÅÊçüÂ§±ÂáΩÊï∞\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GPT2ClassificationModel(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# --- ÊîØÊåÅÔºöÂÜªÁªì GPT2 Ê®°ÂûãÁöÑÂèÇÊï∞ÔºåÊàñËÄÖÂè™ËÆ≠ÁªÉÊüêÂá†Â±Ç ---\n",
        "# for param in model.gpt2.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 2. ËÆ≠ÁªÉÂæ™ÁéØ\n",
        "progress_bar = tqdm(range(num_training_steps),bar_format=custom_format)\n",
        "train(\n",
        "    model = model,\n",
        "    train_loader=train_loader ,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device,\n",
        "    progress_bar=progress_bar\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##È™åËØÅ\n"
      ],
      "metadata": {
        "id": "gHT_7IXUUk1W"
      },
      "id": "gHT_7IXUUk1W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÊµãËØï"
      ],
      "metadata": {
        "id": "Z4m62S0_UwC_"
      },
      "id": "Z4m62S0_UwC_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBjVhZLuwRT"
      },
      "id": "ZOBjVhZLuwRT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e829ff0f1a14f86a50f5f8e644d07d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00c7c8541fcc4ae3b8b23b81aa121056",
              "IPY_MODEL_5daebed8085149e99ea573708cf42008",
              "IPY_MODEL_5acd5c08ddf94a1fbe63427ed1459c60"
            ],
            "layout": "IPY_MODEL_7c86ac1f35e84cfba4737a2e2bf353ae"
          }
        },
        "00c7c8541fcc4ae3b8b23b81aa121056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af031ee173e642b3b81fea951659e1e1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23d8919c426a458d870ef66293e70c40",
            "value": ""
          }
        },
        "5daebed8085149e99ea573708cf42008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b8e2a2e7614a368c3b50dd76a79a32",
            "max": 393,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e66ee5d7083e4d899e754492fb89671f",
            "value": 134
          }
        },
        "5acd5c08ddf94a1fbe63427ed1459c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeb2136943ea4c36877bd2998062933a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e9849d208c44cf7849868df3e007262",
            "value": "‚Äá34%‚Äá|‚Äá134/393‚Äá[00:29&lt;01:38,‚Äá‚Äá2.62it/s]"
          }
        },
        "7c86ac1f35e84cfba4737a2e2bf353ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af031ee173e642b3b81fea951659e1e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d8919c426a458d870ef66293e70c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29b8e2a2e7614a368c3b50dd76a79a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e66ee5d7083e4d899e754492fb89671f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eeb2136943ea4c36877bd2998062933a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9849d208c44cf7849868df3e007262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
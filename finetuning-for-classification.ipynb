{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/finetuning-for-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 进行文本分类的微调"
      ],
      "metadata": {
        "id": "JLfSRs2smELT"
      },
      "id": "JLfSRs2smELT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface gpt2\n",
        "使用[huggingface gpt2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "为了理解 huggingface gpt2的使用，写一个文本生成示例代码，感受下"
      ],
      "metadata": {
        "id": "xLoXMx-zmUCJ"
      },
      "id": "xLoXMx-zmUCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4855611",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f4855611"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(model.config)\n",
        "print('*'*10)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(model.wte.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自定生成文本Model"
      ],
      "metadata": {
        "id": "dC1hF-UgnGxI"
      },
      "id": "dC1hF-UgnGxI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class MyGPT2LMHeadModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.gpt2.wte.weight\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    gpt2_out = self.gpt2(in_idx)\n",
        "    logits = self.lm_head(gpt2_out.last_hidden_state)\n",
        "    return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDvbZlhfbWDM"
      },
      "id": "YDvbZlhfbWDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning,\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用的设备: {device}\")\n",
        "myGPT2LMHeadModel = MyGPT2LMHeadModel()\n",
        "myGPT2LMHeadModel.to(device)\n",
        "token_ids = generate(\n",
        "    model=myGPT2LMHeadModel,\n",
        "    idx= tokenizer.encode(prompt_text, return_tensors='pt').to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=1024,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", tokenizer.decode(token_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHn7gz10x4a",
        "outputId": "293c120c-9a4b-43ce-f0db-3aa7800827d7"
      },
      "id": "RYHn7gz10x4a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用的设备: cuda\n",
            "Output text:\n",
            " In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, he wandered the hallways in her bright red coat and she let him out. Then he moved to play with her and brought her toys, and we spent the next two years in a very kind house in the northern suburbs and around Toronto's Oak Park\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用transformers的GPT2LMHeadModel生成文本"
      ],
      "metadata": {
        "id": "F4YtWaXDnLGk"
      },
      "id": "F4YtWaXDnLGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1203d226",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1203d226"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 检查是否有可用的 GPU，并设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用的设备: {device}\")\n",
        "\n",
        "# 1. 加载带有语言模型头的模型和分词器\n",
        "# 将模型移动到指定的设备上\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "modelLMHead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "modelLMHead.to(device)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# 4. 使用 model.generate() 生成文本\n",
        "print(\"\\n正在生成故事...\")\n",
        "# 调用 generate 方法来创作故事\n",
        "output_sequences = modelLMHead.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=50,          # 生成文本的最大长度（包含提示）\n",
        "    num_return_sequences=1,  # 生成几个不同的故事\n",
        "    no_repeat_ngram_size=2,  # 避免重复短语的关键参数\n",
        "    do_sample=True,          # 启用采样，让文本更有创意，而不是死板的预测\n",
        "    temperature=0.9,         # 控制创造性与确定性的平衡，数值越低越保守\n",
        "    top_k=50,                # 采样时只考虑概率最高的50个词\n",
        "    top_p=0.95,              # 核心采样，保留概率总和为95%的词汇\n",
        ")\n",
        "\n",
        "# 5. 解码生成的文本\n",
        "# 将生成的数字ID序列转换回人类可读的字符串\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. 打印结果\n",
        "print(\"\\n--- 生成的故事 ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuning-for-classification"
      ],
      "metadata": {
        "id": "2pgzANIXoXIO"
      },
      "id": "2pgzANIXoXIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 如果您具有机器学习的背景，对于分类微调您可能已经熟悉. 举个例子，分类微调类似于训练卷积网络来对手写数字进行分类的过程\n",
        "- 在分类微调中，模型可以输出特定的分类标签（例如，“spam”和“not spam”）\n",
        "- 分类微调模型只能预测它在训练期间所熟知的类别标签（例如，“垃圾邮件”或“非垃圾邮件”），而指令微调模型通常可以执行更广泛的任务\n",
        "- 我们可以将分类微调模型视为高度专业化的模型;在实践中，开发专业化的模型通常比开发在许多不同任务上表现良好的通用模型要容易得多"
      ],
      "metadata": {
        "id": "uCbfj-l1znk-"
      },
      "id": "uCbfj-l1znk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###准备数据集\n",
        "我们使用由垃圾邮件和非垃圾邮件组成的数据集来对 LLM 进行分类微调"
      ],
      "metadata": {
        "id": "MT8VVyPtzuLT"
      },
      "id": "MT8VVyPtzuLT"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# === 1. 全局配置 ===\n",
        "URL = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "ZIP_PATH = \"sms_spam_collection.zip\"\n",
        "DATA_DIR = Path(\"sms_spam_collection\")\n",
        "DATA_FILE = DATA_DIR / \"SMSSpamCollection.tsv\"\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. 数据准备 ===\n",
        "def prepare_data():\n",
        "    if not DATA_FILE.exists():\n",
        "        print(\"⬇️ Downloading and extracting dataset...\")\n",
        "        with urllib.request.urlopen(URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        os.rename(DATA_DIR / \"SMSSpamCollection\", DATA_FILE)\n",
        "    else:\n",
        "        print(\"✅ Dataset already exists.\")\n",
        "\n",
        "    df = pd.read_csv(DATA_FILE, sep=\"\\t\", names=[\"Label\", \"Text\"])\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(df[\"Label\"].value_counts())\n",
        "\n",
        "    # 平衡数据集，ham:4825 ,spam:747， 使每个类别包含 747 个实例。\n",
        "    ham = df[df[\"Label\"] == \"ham\"].sample(n=df.Label.value_counts()[\"spam\"], random_state=RANDOM_STATE)\n",
        "    df = pd.concat([ham, df[df[\"Label\"] == \"spam\"]])\n",
        "    df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    return df[:int(0.7*n)], df[int(0.7*n):int(0.8*n)], df[int(0.8*n):]\n",
        "\n",
        "\n",
        "# === 3. Tokenizer & Dataset 使用 Hugging Face `datasets` 库 ===\n",
        "def build_datasets(train_df, val_df, test_df):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    datasets = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"validation\": Dataset.from_pandas(val_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    print(\"\\n🔢 Calculating max sequence length (may take a few seconds)...\")\n",
        "    train_texts = datasets[\"train\"][\"Text\"]\n",
        "    max_len = max(len(tokenizer.encode(t)) for t in tqdm(train_texts))\n",
        "    print(f\"Max length: {max_len}\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "\n",
        "    print(\"\\n✂️ Tokenizing datasets...\")\n",
        "    tokenized = datasets.map(tokenize_fn, batched=True, remove_columns=[\"Text\"])\n",
        "    tokenized = tokenized.rename_column(\"Label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[ \"input_ids\", \"attention_mask\",\"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# === 4. DataLoader ===\n",
        "def create_loaders(datasets):\n",
        "    return {\n",
        "        split: DataLoader(ds, batch_size=BATCH_SIZE,  shuffle=(split == \"train\"),num_workers=NUM_WORKERS)\n",
        "        for split, ds in datasets.items()\n",
        "    }\n",
        "\n",
        "\n",
        "# === 主流程 ===\n",
        "print(\"🏗 Preparing data...\")\n",
        "train_df, val_df, test_df = prepare_data()\n",
        "datasets = build_datasets(train_df, val_df, test_df)\n",
        "loaders = create_loaders(datasets)\n",
        "\n",
        "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"validation\"], loaders[\"test\"]\n",
        "\n",
        "print(\"\\n✅ Data pipeline ready!\")\n",
        "for name, loader in loaders.items():\n",
        "    print(f\"{name:>10}: {len(loader)} batches\")\n",
        "\n",
        "# === 示例输出 ===\n",
        "print(\"\\n📦 Example batch from train_loader:\")\n",
        "batch = next(iter(loaders[\"train\"]))\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k:>15}:\", v.shape)\n",
        "\n",
        "# 查看实际文本\n",
        "print(\"\\n📝 Decoded sample text:\")\n",
        "sample_ids = batch[\"input_ids\"][0]\n",
        "decoded = GPT2Tokenizer.from_pretrained(\"gpt2\").decode(sample_ids, skip_special_tokens=True)\n",
        "print(decoded)\n",
        "print(\"Label:\", batch[\"labels\"][0].item())\n"
      ],
      "metadata": {
        "id": "ntTbiSLy4sGi"
      },
      "id": "ntTbiSLy4sGi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "KTYb7GO9SWjd"
      },
      "id": "KTYb7GO9SWjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型与训练"
      ],
      "metadata": {
        "id": "mT-AD-VLUrfI"
      },
      "id": "mT-AD-VLUrfI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6093a7ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6093a7ed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "def train(model, train_loader,val_loader, optimizer,loss_fn, lr_scheduler, device,progress_bar):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(outputs, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    # 3. 评估循环\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0  # 初始化总损失\n",
        "\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(logits, batch[\"labels\"])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "        total_samples += len(batch[\"labels\"])\n",
        "\n",
        "    # 计算平均损失和准确率\n",
        "    avg_val_loss = total_loss / len(val_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. 初始化模型、优化器、损失函数\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GPT2ClassificationModel(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# --- 支持：冻结 GPT2 模型的参数，或者只训练某几层 ---\n",
        "# for param in model.gpt2.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 2. 训练循环\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "train(\n",
        "    model = model,\n",
        "    train_loader=train_loader ,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device,\n",
        "    progress_bar=progress_bar\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##验证\n"
      ],
      "metadata": {
        "id": "gHT_7IXUUk1W"
      },
      "id": "gHT_7IXUUk1W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 测试"
      ],
      "metadata": {
        "id": "Z4m62S0_UwC_"
      },
      "id": "Z4m62S0_UwC_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBjVhZLuwRT"
      },
      "id": "ZOBjVhZLuwRT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/finetuning-for-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„å¾®è°ƒ"
      ],
      "metadata": {
        "id": "JLfSRs2smELT"
      },
      "id": "JLfSRs2smELT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_any\"\n",
        "login(token=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "#æ ¡éªŒæ˜¯å¦è®¾ç½®æˆåŠŸ\n",
        "! echo $HF_TOKEN\n",
        "```"
      ],
      "metadata": {
        "id": "ZFHQ1SfVpWjN"
      },
      "id": "ZFHQ1SfVpWjN"
    },
    {
      "cell_type": "code",
      "source": [
        "##å±è”½è¿›åº¦æ¡ï¼Œgithubä¸­ä¸æ”¯æŒæ˜¾ç¤ºï¼Œæ•´ä¸ªnotebookéƒ½ä¸æ˜¾ç¤ºäº†\n",
        "import os\n",
        "# è®¾ç½®è¿™ä¸ªç¯å¢ƒå˜é‡æ¥ç¦ç”¨tqdmè¿›åº¦æ¡\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "7LU4K6Ceqde6"
      },
      "id": "7LU4K6Ceqde6",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface gpt2\n",
        "ä½¿ç”¨[huggingface gpt2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "ä¸ºäº†ç†è§£ huggingface gpt2çš„ä½¿ç”¨ï¼Œå†™ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ä»£ç ï¼Œæ„Ÿå—ä¸‹"
      ],
      "metadata": {
        "id": "xLoXMx-zmUCJ"
      },
      "id": "xLoXMx-zmUCJ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f4855611",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4855611",
        "outputId": "0cc6fcd8-ba8d-48a2-d7ee-c0fb0b54c83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "**********\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(model.config)\n",
        "print('*'*10)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(model.wte.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è‡ªå®šç”Ÿæˆæ–‡æœ¬Model"
      ],
      "metadata": {
        "id": "dC1hF-UgnGxI"
      },
      "id": "dC1hF-UgnGxI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class MyGPT2LMHeadModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.gpt2.wte.weight\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    gpt2_out = self.gpt2(in_idx)\n",
        "    logits = self.lm_head(gpt2_out.last_hidden_state)\n",
        "    return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDvbZlhfbWDM"
      },
      "id": "YDvbZlhfbWDM",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning,\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
        "myGPT2LMHeadModel = MyGPT2LMHeadModel()\n",
        "myGPT2LMHeadModel.to(device)\n",
        "token_ids = generate(\n",
        "    model=myGPT2LMHeadModel,\n",
        "    idx= tokenizer.encode(prompt_text, return_tensors='pt').to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=1024,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", tokenizer.decode(token_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHn7gz10x4a",
        "outputId": "e870238d-f5db-47ef-8fe8-3c6ea96a9620"
      },
      "id": "RYHn7gz10x4a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨çš„è®¾å¤‡: cuda\n",
            "Output text:\n",
            " In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, he and his cat, Kitten, had a hard time sleeping. Their cat, a baby kitten, could not wake up but was fine after a night of fun and play. Soon afterward, Kitten's parents were back at home with him,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä½¿ç”¨transformersçš„GPT2LMHeadModelç”Ÿæˆæ–‡æœ¬"
      ],
      "metadata": {
        "id": "F4YtWaXDnLGk"
      },
      "id": "F4YtWaXDnLGk"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1203d226",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1203d226",
        "outputId": "5c1ddd35-e47c-41d5-cff1-c149954f0d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨çš„è®¾å¤‡: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æ­£åœ¨ç”Ÿæˆæ•…äº‹...\n",
            "\n",
            "--- ç”Ÿæˆçš„æ•…äº‹ ---\n",
            "In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, Whizzers noticed Whiskey's fluffy coat. He and Whisky walked to the cottage and found Whistle sleeping in Whisp, a small room\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPUï¼Œå¹¶è®¾ç½®è®¾å¤‡\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
        "\n",
        "# 1. åŠ è½½å¸¦æœ‰è¯­è¨€æ¨¡å‹å¤´çš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "modelLMHead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "modelLMHead.to(device)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# 4. ä½¿ç”¨ model.generate() ç”Ÿæˆæ–‡æœ¬\n",
        "print(\"\\næ­£åœ¨ç”Ÿæˆæ•…äº‹...\")\n",
        "# è°ƒç”¨ generate æ–¹æ³•æ¥åˆ›ä½œæ•…äº‹\n",
        "output_sequences = modelLMHead.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=50,          # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆåŒ…å«æç¤ºï¼‰\n",
        "    num_return_sequences=1,  # ç”Ÿæˆå‡ ä¸ªä¸åŒçš„æ•…äº‹\n",
        "    no_repeat_ngram_size=2,  # é¿å…é‡å¤çŸ­è¯­çš„å…³é”®å‚æ•°\n",
        "    do_sample=True,          # å¯ç”¨é‡‡æ ·ï¼Œè®©æ–‡æœ¬æ›´æœ‰åˆ›æ„ï¼Œè€Œä¸æ˜¯æ­»æ¿çš„é¢„æµ‹\n",
        "    temperature=0.9,         # æ§åˆ¶åˆ›é€ æ€§ä¸ç¡®å®šæ€§çš„å¹³è¡¡ï¼Œæ•°å€¼è¶Šä½è¶Šä¿å®ˆ\n",
        "    top_k=50,                # é‡‡æ ·æ—¶åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„50ä¸ªè¯\n",
        "    top_p=0.95,              # æ ¸å¿ƒé‡‡æ ·ï¼Œä¿ç•™æ¦‚ç‡æ€»å’Œä¸º95%çš„è¯æ±‡\n",
        ")\n",
        "\n",
        "# 5. è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
        "# å°†ç”Ÿæˆçš„æ•°å­—IDåºåˆ—è½¬æ¢å›äººç±»å¯è¯»çš„å­—ç¬¦ä¸²\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. æ‰“å°ç»“æœ\n",
        "print(\"\\n--- ç”Ÿæˆçš„æ•…äº‹ ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuning-for-classification"
      ],
      "metadata": {
        "id": "2pgzANIXoXIO"
      },
      "id": "2pgzANIXoXIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- å¦‚æœæ‚¨å…·æœ‰æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ï¼Œå¯¹äºåˆ†ç±»å¾®è°ƒæ‚¨å¯èƒ½å·²ç»ç†Ÿæ‚‰. ä¸¾ä¸ªä¾‹å­ï¼Œåˆ†ç±»å¾®è°ƒç±»ä¼¼äºè®­ç»ƒå·ç§¯ç½‘ç»œæ¥å¯¹æ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»çš„è¿‡ç¨‹\n",
        "- åœ¨åˆ†ç±»å¾®è°ƒä¸­ï¼Œæ¨¡å‹å¯ä»¥è¾“å‡ºç‰¹å®šçš„åˆ†ç±»æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œspamâ€å’Œâ€œnot spamâ€ï¼‰\n",
        "- åˆ†ç±»å¾®è°ƒæ¨¡å‹åªèƒ½é¢„æµ‹å®ƒåœ¨è®­ç»ƒæœŸé—´æ‰€ç†ŸçŸ¥çš„ç±»åˆ«æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œåƒåœ¾é‚®ä»¶â€æˆ–â€œéåƒåœ¾é‚®ä»¶â€ï¼‰ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹é€šå¸¸å¯ä»¥æ‰§è¡Œæ›´å¹¿æ³›çš„ä»»åŠ¡\n",
        "- æˆ‘ä»¬å¯ä»¥å°†åˆ†ç±»å¾®è°ƒæ¨¡å‹è§†ä¸ºé«˜åº¦ä¸“ä¸šåŒ–çš„æ¨¡å‹;åœ¨å®è·µä¸­ï¼Œå¼€å‘ä¸“ä¸šåŒ–çš„æ¨¡å‹é€šå¸¸æ¯”å¼€å‘åœ¨è®¸å¤šä¸åŒä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„é€šç”¨æ¨¡å‹è¦å®¹æ˜“å¾—å¤š"
      ],
      "metadata": {
        "id": "uCbfj-l1znk-"
      },
      "id": "uCbfj-l1znk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###å‡†å¤‡æ•°æ®é›†\n",
        "æˆ‘ä»¬ä½¿ç”¨ç”±åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶ç»„æˆçš„æ•°æ®é›†æ¥å¯¹ LLM è¿›è¡Œåˆ†ç±»å¾®è°ƒ"
      ],
      "metadata": {
        "id": "MT8VVyPtzuLT"
      },
      "id": "MT8VVyPtzuLT"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# === 1. å…¨å±€é…ç½® ===\n",
        "URL = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "ZIP_PATH = \"sms_spam_collection.zip\"\n",
        "DATA_DIR = Path(\"sms_spam_collection\")\n",
        "DATA_FILE = DATA_DIR / \"SMSSpamCollection.tsv\"\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. æ•°æ®å‡†å¤‡ ===\n",
        "def prepare_data():\n",
        "    if not DATA_FILE.exists():\n",
        "        print(\"â¬‡ï¸ Downloading and extracting dataset...\")\n",
        "        with urllib.request.urlopen(URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        os.rename(DATA_DIR / \"SMSSpamCollection\", DATA_FILE)\n",
        "    else:\n",
        "        print(\"âœ… Dataset already exists.\")\n",
        "\n",
        "    df = pd.read_csv(DATA_FILE, sep=\"\\t\", names=[\"Label\", \"Text\"])\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(df[\"Label\"].value_counts())\n",
        "\n",
        "    # å¹³è¡¡æ•°æ®é›†ï¼Œham:4825 ,spam:747ï¼Œ ä½¿æ¯ä¸ªç±»åˆ«åŒ…å« 747 ä¸ªå®ä¾‹ã€‚\n",
        "    ham = df[df[\"Label\"] == \"ham\"].sample(n=df.Label.value_counts()[\"spam\"], random_state=RANDOM_STATE)\n",
        "    df = pd.concat([ham, df[df[\"Label\"] == \"spam\"]])\n",
        "    df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    return df[:int(0.7*n)], df[int(0.7*n):int(0.8*n)], df[int(0.8*n):]\n",
        "\n",
        "\n",
        "# === 3. Tokenizer & Dataset ä½¿ç”¨ Hugging Face `datasets` åº“ ===\n",
        "def build_datasetDict(train_df, val_df, test_df):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    datasetDict = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"validation\": Dataset.from_pandas(val_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    print(\"\\nğŸ”¢ Calculating max sequence length (may take a few seconds)...\")\n",
        "    train_texts = datasetDict[\"train\"][\"Text\"]\n",
        "\n",
        "    custom_format = \"{desc}: {percentage:3.0f}% | {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
        "    max_len = max(len(tokenizer.encode(t)) for t in tqdm(train_texts,bar_format=custom_format))\n",
        "    print(f\"Max length: {max_len}\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "\n",
        "    print(\"\\nâœ‚ï¸ Tokenizing datasets...\")\n",
        "    tokenized = datasetDict.map(tokenize_fn, batched=True, remove_columns=[\"Text\"])\n",
        "    tokenized = tokenized.rename_column(\"Label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[ \"input_ids\", \"attention_mask\",\"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# === 4. DataLoader ===\n",
        "def create_loaders(datasetDict):\n",
        "    return {\n",
        "        split: DataLoader(ds, batch_size=BATCH_SIZE,  shuffle=(split == \"train\"),num_workers=NUM_WORKERS)\n",
        "        for split, ds in datasetDict.items()\n",
        "    }\n",
        "\n",
        "\n",
        "# === ä¸»æµç¨‹ ===\n",
        "print(\"ğŸ— Preparing data...\")\n",
        "train_df, val_df, test_df = prepare_data()\n",
        "datasetDict = build_datasetDict(train_df, val_df, test_df)\n",
        "loaders = create_loaders(datasetDict)\n",
        "\n",
        "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"validation\"], loaders[\"test\"]\n",
        "\n",
        "print(\"\\nâœ… Data pipeline ready!\")\n",
        "for name, loader in loaders.items():\n",
        "    print(f\"{name:>10}: {len(loader)} batches\")\n",
        "\n",
        "# === ç¤ºä¾‹è¾“å‡º ===\n",
        "print(\"\\nğŸ“¦ Example batch from train_loader:\")\n",
        "batch = next(iter(loaders[\"train\"]))\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k:>15}:\", v.shape)\n",
        "\n",
        "# æŸ¥çœ‹å®é™…æ–‡æœ¬\n",
        "print(\"\\nğŸ“ Decoded sample text:\")\n",
        "sample_ids = batch[\"input_ids\"][0]\n",
        "decoded = GPT2Tokenizer.from_pretrained(\"gpt2\").decode(sample_ids, skip_special_tokens=True)\n",
        "print(decoded)\n",
        "print(\"Label:\", batch[\"labels\"][0].item())\n"
      ],
      "metadata": {
        "id": "ntTbiSLy4sGi",
        "outputId": "e390f8c5-4c4e-435b-e06c-35c4388a7eb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ntTbiSLy4sGi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ— Preparing data...\n",
            "â¬‡ï¸ Downloading and extracting dataset...\n",
            "Loaded 5572 samples\n",
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸ”¢ Calculating max sequence length (may take a few seconds)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% | 1045/1045 [00:00<00:00, 2412.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 120\n",
            "\n",
            "âœ‚ï¸ Tokenizing datasets...\n",
            "\n",
            "âœ… Data pipeline ready!\n",
            "     train: 131 batches\n",
            "validation: 19 batches\n",
            "      test: 38 batches\n",
            "\n",
            "ğŸ“¦ Example batch from train_loader:\n",
            "         labels: torch.Size([8])\n",
            "      input_ids: torch.Size([8, 120])\n",
            " attention_mask: torch.Size([8, 120])\n",
            "\n",
            "ğŸ“ Decoded sample text:\n",
            "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a Â£1500 Bonus Prize, call 09066364589\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "KTYb7GO9SWjd",
        "outputId": "455d6cd6-28ce-41a7-a9a3-4ec580356236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KTYb7GO9SWjd",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¨¡å‹"
      ],
      "metadata": {
        "id": "mT-AD-VLUrfI"
      },
      "id": "mT-AD-VLUrfI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "c2fmIPa0AnQd"
      },
      "id": "c2fmIPa0AnQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è®­ç»ƒä¸éªŒè¯"
      ],
      "metadata": {
        "id": "7blEF0TiAjmk"
      },
      "id": "7blEF0TiAjmk"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6093a7ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6093a7ed",
        "outputId": "24a1c01c-40ac-4e70-e515-f6a4428e5c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 131/131 , Elapsed=27.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3.        | TrainLoss=0.2210 | ValLoss=0.0420 | Acc=0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 131/131 , Elapsed=27.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3.        | TrainLoss=0.0386 | ValLoss=0.1073 | Acc=0.9800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 131/131 , Elapsed=26.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3.        | TrainLoss=0.0110 | ValLoss=0.0563 | Acc=0.9867\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "def train(model, train_loader,val_loader, optimizer,loss_fn, lr_scheduler, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        total_train_loss = 0  # åˆå§‹åŒ–æ€»æŸå¤±\n",
        "        for batch in progress_bar:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(outputs, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_train_loss += loss.item()\n",
        "            elapsed = time.time() - start_time\n",
        "            progress_bar.set_postfix({\"Elapsed\": f\"{elapsed:.2f}s\"})\n",
        "\n",
        "        # 3. è¯„ä¼°å¾ªç¯\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        total_loss = 0  # åˆå§‹åŒ–æ€»æŸå¤±\n",
        "\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"])\n",
        "                loss = loss_fn(logits, batch[\"labels\"])\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total_samples += len(batch[\"labels\"])\n",
        "\n",
        "        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "        avg_val_loss = total_loss / len(val_loader)\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # tqdm.write ä¸ä¼šç ´åè¿›åº¦æ¡çš„å•è¡Œæ˜¾ç¤º\n",
        "        tqdm.write(\n",
        "            f\"Epoch {epoch+1}/{num_epochs}.                | \"\n",
        "            f\"TrainLoss={avg_train_loss:.4f} | \"\n",
        "            f\"ValLoss={avg_val_loss:.4f} | \"\n",
        "            f\"Acc={accuracy:.4f}\"\n",
        ")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GPT2ClassificationModel(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# --- æ”¯æŒï¼šå†»ç»“ GPT2 æ¨¡å‹çš„å‚æ•°ï¼Œæˆ–è€…åªè®­ç»ƒæŸå‡ å±‚ ---\n",
        "# for param in model.gpt2.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 2. è®­ç»ƒå¾ªç¯\n",
        "train(\n",
        "    model = model,\n",
        "    train_loader=train_loader ,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##é¢„æµ‹\n"
      ],
      "metadata": {
        "id": "gHT_7IXUUk1W"
      },
      "id": "gHT_7IXUUk1W"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBjVhZLuwRT"
      },
      "id": "ZOBjVhZLuwRT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/finetuning-for-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 进行文本分类的微调"
      ],
      "metadata": {
        "id": "JLfSRs2smELT"
      },
      "id": "JLfSRs2smELT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_any\"\n",
        "login(token=os.environ[\"HF_TOKEN\"])\n",
        "\n",
        "#校验是否设置成功\n",
        "! echo $HF_TOKEN\n",
        "```"
      ],
      "metadata": {
        "id": "ZFHQ1SfVpWjN"
      },
      "id": "ZFHQ1SfVpWjN"
    },
    {
      "cell_type": "code",
      "source": [
        "##屏蔽进度条，github中不支持显示，整个notebook都不显示了\n",
        "import os\n",
        "# 设置这个环境变量来禁用tqdm进度条\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import datasets\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "7LU4K6Ceqde6"
      },
      "id": "7LU4K6Ceqde6",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface gpt2\n",
        "使用[huggingface gpt2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "为了理解 huggingface gpt2的使用，写一个文本生成示例代码，感受下"
      ],
      "metadata": {
        "id": "xLoXMx-zmUCJ"
      },
      "id": "xLoXMx-zmUCJ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f4855611",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4855611",
        "outputId": "0cc6fcd8-ba8d-48a2-d7ee-c0fb0b54c83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "**********\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(model.config)\n",
        "print('*'*10)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(model.wte.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自定生成文本Model"
      ],
      "metadata": {
        "id": "dC1hF-UgnGxI"
      },
      "id": "dC1hF-UgnGxI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class MyGPT2LMHeadModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.gpt2.wte.weight\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    gpt2_out = self.gpt2(in_idx)\n",
        "    logits = self.lm_head(gpt2_out.last_hidden_state)\n",
        "    return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDvbZlhfbWDM"
      },
      "id": "YDvbZlhfbWDM",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning,\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用的设备: {device}\")\n",
        "myGPT2LMHeadModel = MyGPT2LMHeadModel()\n",
        "myGPT2LMHeadModel.to(device)\n",
        "token_ids = generate(\n",
        "    model=myGPT2LMHeadModel,\n",
        "    idx= tokenizer.encode(prompt_text, return_tensors='pt').to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=1024,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", tokenizer.decode(token_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHn7gz10x4a",
        "outputId": "e870238d-f5db-47ef-8fe8-3c6ea96a9620"
      },
      "id": "RYHn7gz10x4a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用的设备: cuda\n",
            "Output text:\n",
            " In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, he and his cat, Kitten, had a hard time sleeping. Their cat, a baby kitten, could not wake up but was fine after a night of fun and play. Soon afterward, Kitten's parents were back at home with him,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用transformers的GPT2LMHeadModel生成文本"
      ],
      "metadata": {
        "id": "F4YtWaXDnLGk"
      },
      "id": "F4YtWaXDnLGk"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1203d226",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1203d226",
        "outputId": "5c1ddd35-e47c-41d5-cff1-c149954f0d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用的设备: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "正在生成故事...\n",
            "\n",
            "--- 生成的故事 ---\n",
            "In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, Whizzers noticed Whiskey's fluffy coat. He and Whisky walked to the cottage and found Whistle sleeping in Whisp, a small room\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# 检查是否有可用的 GPU，并设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用的设备: {device}\")\n",
        "\n",
        "# 1. 加载带有语言模型头的模型和分词器\n",
        "# 将模型移动到指定的设备上\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "modelLMHead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "modelLMHead.to(device)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# 4. 使用 model.generate() 生成文本\n",
        "print(\"\\n正在生成故事...\")\n",
        "# 调用 generate 方法来创作故事\n",
        "output_sequences = modelLMHead.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=50,          # 生成文本的最大长度（包含提示）\n",
        "    num_return_sequences=1,  # 生成几个不同的故事\n",
        "    no_repeat_ngram_size=2,  # 避免重复短语的关键参数\n",
        "    do_sample=True,          # 启用采样，让文本更有创意，而不是死板的预测\n",
        "    temperature=0.9,         # 控制创造性与确定性的平衡，数值越低越保守\n",
        "    top_k=50,                # 采样时只考虑概率最高的50个词\n",
        "    top_p=0.95,              # 核心采样，保留概率总和为95%的词汇\n",
        ")\n",
        "\n",
        "# 5. 解码生成的文本\n",
        "# 将生成的数字ID序列转换回人类可读的字符串\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. 打印结果\n",
        "print(\"\\n--- 生成的故事 ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuning-for-classification"
      ],
      "metadata": {
        "id": "2pgzANIXoXIO"
      },
      "id": "2pgzANIXoXIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 如果您具有机器学习的背景，对于分类微调您可能已经熟悉. 举个例子，分类微调类似于训练卷积网络来对手写数字进行分类的过程\n",
        "- 在分类微调中，模型可以输出特定的分类标签（例如，“spam”和“not spam”）\n",
        "- 分类微调模型只能预测它在训练期间所熟知的类别标签（例如，“垃圾邮件”或“非垃圾邮件”），而指令微调模型通常可以执行更广泛的任务\n",
        "- 我们可以将分类微调模型视为高度专业化的模型;在实践中，开发专业化的模型通常比开发在许多不同任务上表现良好的通用模型要容易得多"
      ],
      "metadata": {
        "id": "uCbfj-l1znk-"
      },
      "id": "uCbfj-l1znk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###准备数据集\n",
        "我们使用由垃圾邮件和非垃圾邮件组成的数据集来对 LLM 进行分类微调"
      ],
      "metadata": {
        "id": "MT8VVyPtzuLT"
      },
      "id": "MT8VVyPtzuLT"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# === 1. 全局配置 ===\n",
        "URL = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "ZIP_PATH = \"sms_spam_collection.zip\"\n",
        "DATA_DIR = Path(\"sms_spam_collection\")\n",
        "DATA_FILE = DATA_DIR / \"SMSSpamCollection.tsv\"\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. 数据准备 ===\n",
        "def prepare_data():\n",
        "    if not DATA_FILE.exists():\n",
        "        print(\"⬇️ Downloading and extracting dataset...\")\n",
        "        with urllib.request.urlopen(URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        os.rename(DATA_DIR / \"SMSSpamCollection\", DATA_FILE)\n",
        "    else:\n",
        "        print(\"✅ Dataset already exists.\")\n",
        "\n",
        "    df = pd.read_csv(DATA_FILE, sep=\"\\t\", names=[\"Label\", \"Text\"])\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(df[\"Label\"].value_counts())\n",
        "\n",
        "    # 平衡数据集，ham:4825 ,spam:747， 使每个类别包含 747 个实例。\n",
        "    ham = df[df[\"Label\"] == \"ham\"].sample(n=df.Label.value_counts()[\"spam\"], random_state=RANDOM_STATE)\n",
        "    df = pd.concat([ham, df[df[\"Label\"] == \"spam\"]])\n",
        "    df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    return df[:int(0.7*n)], df[int(0.7*n):int(0.8*n)], df[int(0.8*n):]\n",
        "\n",
        "\n",
        "# === 3. Tokenizer & Dataset 使用 Hugging Face `datasets` 库 ===\n",
        "def build_datasetDict(train_df, val_df, test_df):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    datasetDict = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"validation\": Dataset.from_pandas(val_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    print(\"\\n🔢 Calculating max sequence length (may take a few seconds)...\")\n",
        "    train_texts = datasetDict[\"train\"][\"Text\"]\n",
        "\n",
        "    custom_format = \"{desc}: {percentage:3.0f}% | {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
        "    max_len = max(len(tokenizer.encode(t)) for t in tqdm(train_texts,bar_format=custom_format))\n",
        "    print(f\"Max length: {max_len}\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "\n",
        "    print(\"\\n✂️ Tokenizing datasets...\")\n",
        "    tokenized = datasetDict.map(tokenize_fn, batched=True, remove_columns=[\"Text\"])\n",
        "    tokenized = tokenized.rename_column(\"Label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[ \"input_ids\", \"attention_mask\",\"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# === 4. DataLoader ===\n",
        "def create_loaders(datasetDict):\n",
        "    return {\n",
        "        split: DataLoader(ds, batch_size=BATCH_SIZE,  shuffle=(split == \"train\"),num_workers=NUM_WORKERS)\n",
        "        for split, ds in datasetDict.items()\n",
        "    }\n",
        "\n",
        "\n",
        "# === 主流程 ===\n",
        "print(\"🏗 Preparing data...\")\n",
        "train_df, val_df, test_df = prepare_data()\n",
        "datasetDict = build_datasetDict(train_df, val_df, test_df)\n",
        "loaders = create_loaders(datasetDict)\n",
        "\n",
        "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"validation\"], loaders[\"test\"]\n",
        "\n",
        "print(\"\\n✅ Data pipeline ready!\")\n",
        "for name, loader in loaders.items():\n",
        "    print(f\"{name:>10}: {len(loader)} batches\")\n",
        "\n",
        "# === 示例输出 ===\n",
        "print(\"\\n📦 Example batch from train_loader:\")\n",
        "batch = next(iter(loaders[\"train\"]))\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k:>15}:\", v.shape)\n",
        "\n",
        "# 查看实际文本\n",
        "print(\"\\n📝 Decoded sample text:\")\n",
        "sample_ids = batch[\"input_ids\"][0]\n",
        "decoded = GPT2Tokenizer.from_pretrained(\"gpt2\").decode(sample_ids, skip_special_tokens=True)\n",
        "print(decoded)\n",
        "print(\"Label:\", batch[\"labels\"][0].item())\n"
      ],
      "metadata": {
        "id": "ntTbiSLy4sGi",
        "outputId": "e390f8c5-4c4e-435b-e06c-35c4388a7eb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ntTbiSLy4sGi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏗 Preparing data...\n",
            "⬇️ Downloading and extracting dataset...\n",
            "Loaded 5572 samples\n",
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "\n",
            "🔢 Calculating max sequence length (may take a few seconds)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% | 1045/1045 [00:00<00:00, 2412.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 120\n",
            "\n",
            "✂️ Tokenizing datasets...\n",
            "\n",
            "✅ Data pipeline ready!\n",
            "     train: 131 batches\n",
            "validation: 19 batches\n",
            "      test: 38 batches\n",
            "\n",
            "📦 Example batch from train_loader:\n",
            "         labels: torch.Size([8])\n",
            "      input_ids: torch.Size([8, 120])\n",
            " attention_mask: torch.Size([8, 120])\n",
            "\n",
            "📝 Decoded sample text:\n",
            "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "KTYb7GO9SWjd",
        "outputId": "455d6cd6-28ce-41a7-a9a3-4ec580356236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KTYb7GO9SWjd",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型"
      ],
      "metadata": {
        "id": "mT-AD-VLUrfI"
      },
      "id": "mT-AD-VLUrfI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "c2fmIPa0AnQd"
      },
      "id": "c2fmIPa0AnQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练与验证"
      ],
      "metadata": {
        "id": "7blEF0TiAjmk"
      },
      "id": "7blEF0TiAjmk"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6093a7ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6093a7ed",
        "outputId": "24a1c01c-40ac-4e70-e515-f6a4428e5c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 131/131 , Elapsed=27.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3.        | TrainLoss=0.2210 | ValLoss=0.0420 | Acc=0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 131/131 , Elapsed=27.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3.        | TrainLoss=0.0386 | ValLoss=0.1073 | Acc=0.9800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 131/131 , Elapsed=26.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3.        | TrainLoss=0.0110 | ValLoss=0.0563 | Acc=0.9867\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "def train(model, train_loader,val_loader, optimizer,loss_fn, lr_scheduler, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        progress_bar = tqdm(train_loader,\n",
        "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            bar_format=\"{desc}: {n_fmt}/{total_fmt} {postfix}\")\n",
        "\n",
        "        total_train_loss = 0  # 初始化总损失\n",
        "        for batch in progress_bar:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(outputs, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_train_loss += loss.item()\n",
        "            elapsed = time.time() - start_time\n",
        "            progress_bar.set_postfix({\"Elapsed\": f\"{elapsed:.2f}s\"})\n",
        "\n",
        "        # 3. 评估循环\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        total_loss = 0  # 初始化总损失\n",
        "\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "                logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"])\n",
        "                loss = loss_fn(logits, batch[\"labels\"])\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total_samples += len(batch[\"labels\"])\n",
        "\n",
        "        # 计算平均损失和准确率\n",
        "        avg_val_loss = total_loss / len(val_loader)\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # tqdm.write 不会破坏进度条的单行显示\n",
        "        tqdm.write(\n",
        "            f\"Epoch {epoch+1}/{num_epochs}.                | \"\n",
        "            f\"TrainLoss={avg_train_loss:.4f} | \"\n",
        "            f\"ValLoss={avg_val_loss:.4f} | \"\n",
        "            f\"Acc={accuracy:.4f}\"\n",
        ")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. 初始化模型、优化器、损失函数\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GPT2ClassificationModel(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# --- 支持：冻结 GPT2 模型的参数，或者只训练某几层 ---\n",
        "# for param in model.gpt2.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 2. 训练循环\n",
        "train(\n",
        "    model = model,\n",
        "    train_loader=train_loader ,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##预测\n"
      ],
      "metadata": {
        "id": "gHT_7IXUUk1W"
      },
      "id": "gHT_7IXUUk1W"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBjVhZLuwRT"
      },
      "id": "ZOBjVhZLuwRT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
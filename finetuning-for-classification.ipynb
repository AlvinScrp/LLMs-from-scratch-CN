{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/LLMs-from-scratch-CN/blob/main/finetuning-for-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„å¾®è°ƒ"
      ],
      "metadata": {
        "id": "JLfSRs2smELT"
      },
      "id": "JLfSRs2smELT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface gpt2\n",
        "ä½¿ç”¨[huggingface gpt2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "ä¸ºäº†ç†è§£ huggingface gpt2çš„ä½¿ç”¨ï¼Œå†™ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ä»£ç ï¼Œæ„Ÿå—ä¸‹"
      ],
      "metadata": {
        "id": "xLoXMx-zmUCJ"
      },
      "id": "xLoXMx-zmUCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4855611",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f4855611"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(model.config)\n",
        "print('*'*10)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(model.wte.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è‡ªå®šç”Ÿæˆæ–‡æœ¬Model"
      ],
      "metadata": {
        "id": "dC1hF-UgnGxI"
      },
      "id": "dC1hF-UgnGxI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class MyGPT2LMHeadModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.lm_head.weight = self.gpt2.wte.weight\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    gpt2_out = self.gpt2(in_idx)\n",
        "    logits = self.lm_head(gpt2_out.last_hidden_state)\n",
        "    return logits\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDvbZlhfbWDM"
      },
      "id": "YDvbZlhfbWDM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning,\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
        "myGPT2LMHeadModel = MyGPT2LMHeadModel()\n",
        "myGPT2LMHeadModel.to(device)\n",
        "token_ids = generate(\n",
        "    model=myGPT2LMHeadModel,\n",
        "    idx= tokenizer.encode(prompt_text, return_tensors='pt').to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=1024,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", tokenizer.decode(token_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYHn7gz10x4a",
        "outputId": "293c120c-9a4b-43ce-f0db-3aa7800827d7"
      },
      "id": "RYHn7gz10x4a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨çš„è®¾å¤‡: cuda\n",
            "Output text:\n",
            " In a cozy little cottage, lived a fluffy cat named Whiskers. One sunny morning, he wandered the hallways in her bright red coat and she let him out. Then he moved to play with her and brought her toys, and we spent the next two years in a very kind house in the northern suburbs and around Toronto's Oak Park\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä½¿ç”¨transformersçš„GPT2LMHeadModelç”Ÿæˆæ–‡æœ¬"
      ],
      "metadata": {
        "id": "F4YtWaXDnLGk"
      },
      "id": "F4YtWaXDnLGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1203d226",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1203d226"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPUï¼Œå¹¶è®¾ç½®è®¾å¤‡\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
        "\n",
        "# 1. åŠ è½½å¸¦æœ‰è¯­è¨€æ¨¡å‹å¤´çš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "modelLMHead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "modelLMHead.to(device)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# 4. ä½¿ç”¨ model.generate() ç”Ÿæˆæ–‡æœ¬\n",
        "print(\"\\næ­£åœ¨ç”Ÿæˆæ•…äº‹...\")\n",
        "# è°ƒç”¨ generate æ–¹æ³•æ¥åˆ›ä½œæ•…äº‹\n",
        "output_sequences = modelLMHead.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=50,          # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆåŒ…å«æç¤ºï¼‰\n",
        "    num_return_sequences=1,  # ç”Ÿæˆå‡ ä¸ªä¸åŒçš„æ•…äº‹\n",
        "    no_repeat_ngram_size=2,  # é¿å…é‡å¤çŸ­è¯­çš„å…³é”®å‚æ•°\n",
        "    do_sample=True,          # å¯ç”¨é‡‡æ ·ï¼Œè®©æ–‡æœ¬æ›´æœ‰åˆ›æ„ï¼Œè€Œä¸æ˜¯æ­»æ¿çš„é¢„æµ‹\n",
        "    temperature=0.9,         # æ§åˆ¶åˆ›é€ æ€§ä¸ç¡®å®šæ€§çš„å¹³è¡¡ï¼Œæ•°å€¼è¶Šä½è¶Šä¿å®ˆ\n",
        "    top_k=50,                # é‡‡æ ·æ—¶åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„50ä¸ªè¯\n",
        "    top_p=0.95,              # æ ¸å¿ƒé‡‡æ ·ï¼Œä¿ç•™æ¦‚ç‡æ€»å’Œä¸º95%çš„è¯æ±‡\n",
        ")\n",
        "\n",
        "# 5. è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
        "# å°†ç”Ÿæˆçš„æ•°å­—IDåºåˆ—è½¬æ¢å›äººç±»å¯è¯»çš„å­—ç¬¦ä¸²\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. æ‰“å°ç»“æœ\n",
        "print(\"\\n--- ç”Ÿæˆçš„æ•…äº‹ ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuning-for-classification"
      ],
      "metadata": {
        "id": "2pgzANIXoXIO"
      },
      "id": "2pgzANIXoXIO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- å¦‚æœæ‚¨å…·æœ‰æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ï¼Œå¯¹äºåˆ†ç±»å¾®è°ƒæ‚¨å¯èƒ½å·²ç»ç†Ÿæ‚‰. ä¸¾ä¸ªä¾‹å­ï¼Œåˆ†ç±»å¾®è°ƒç±»ä¼¼äºè®­ç»ƒå·ç§¯ç½‘ç»œæ¥å¯¹æ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»çš„è¿‡ç¨‹\n",
        "- åœ¨åˆ†ç±»å¾®è°ƒä¸­ï¼Œæ¨¡å‹å¯ä»¥è¾“å‡ºç‰¹å®šçš„åˆ†ç±»æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œspamâ€å’Œâ€œnot spamâ€ï¼‰\n",
        "- åˆ†ç±»å¾®è°ƒæ¨¡å‹åªèƒ½é¢„æµ‹å®ƒåœ¨è®­ç»ƒæœŸé—´æ‰€ç†ŸçŸ¥çš„ç±»åˆ«æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œåƒåœ¾é‚®ä»¶â€æˆ–â€œéåƒåœ¾é‚®ä»¶â€ï¼‰ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹é€šå¸¸å¯ä»¥æ‰§è¡Œæ›´å¹¿æ³›çš„ä»»åŠ¡\n",
        "- æˆ‘ä»¬å¯ä»¥å°†åˆ†ç±»å¾®è°ƒæ¨¡å‹è§†ä¸ºé«˜åº¦ä¸“ä¸šåŒ–çš„æ¨¡å‹;åœ¨å®è·µä¸­ï¼Œå¼€å‘ä¸“ä¸šåŒ–çš„æ¨¡å‹é€šå¸¸æ¯”å¼€å‘åœ¨è®¸å¤šä¸åŒä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„é€šç”¨æ¨¡å‹è¦å®¹æ˜“å¾—å¤š"
      ],
      "metadata": {
        "id": "uCbfj-l1znk-"
      },
      "id": "uCbfj-l1znk-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###å‡†å¤‡æ•°æ®é›†\n",
        "æˆ‘ä»¬ä½¿ç”¨ç”±åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶ç»„æˆçš„æ•°æ®é›†æ¥å¯¹ LLM è¿›è¡Œåˆ†ç±»å¾®è°ƒ"
      ],
      "metadata": {
        "id": "MT8VVyPtzuLT"
      },
      "id": "MT8VVyPtzuLT"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# === 1. å…¨å±€é…ç½® ===\n",
        "URL = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "ZIP_PATH = \"sms_spam_collection.zip\"\n",
        "DATA_DIR = Path(\"sms_spam_collection\")\n",
        "DATA_FILE = DATA_DIR / \"SMSSpamCollection.tsv\"\n",
        "BATCH_SIZE = 8\n",
        "RANDOM_STATE = 123\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# === 2. æ•°æ®å‡†å¤‡ ===\n",
        "def prepare_data():\n",
        "    if not DATA_FILE.exists():\n",
        "        print(\"â¬‡ï¸ Downloading and extracting dataset...\")\n",
        "        with urllib.request.urlopen(URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
        "            f.write(r.read())\n",
        "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        os.rename(DATA_DIR / \"SMSSpamCollection\", DATA_FILE)\n",
        "    else:\n",
        "        print(\"âœ… Dataset already exists.\")\n",
        "\n",
        "    df = pd.read_csv(DATA_FILE, sep=\"\\t\", names=[\"Label\", \"Text\"])\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(df[\"Label\"].value_counts())\n",
        "\n",
        "    # å¹³è¡¡æ•°æ®é›†ï¼Œham:4825 ,spam:747ï¼Œ ä½¿æ¯ä¸ªç±»åˆ«åŒ…å« 747 ä¸ªå®ä¾‹ã€‚\n",
        "    ham = df[df[\"Label\"] == \"ham\"].sample(n=df.Label.value_counts()[\"spam\"], random_state=RANDOM_STATE)\n",
        "    df = pd.concat([ham, df[df[\"Label\"] == \"spam\"]])\n",
        "    df[\"Label\"] = df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    return df[:int(0.7*n)], df[int(0.7*n):int(0.8*n)], df[int(0.8*n):]\n",
        "\n",
        "\n",
        "# === 3. Tokenizer & Dataset ä½¿ç”¨ Hugging Face `datasets` åº“ ===\n",
        "def build_datasets(train_df, val_df, test_df):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    datasets = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"validation\": Dataset.from_pandas(val_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    print(\"\\nğŸ”¢ Calculating max sequence length (may take a few seconds)...\")\n",
        "    train_texts = datasets[\"train\"][\"Text\"]\n",
        "    max_len = max(len(tokenizer.encode(t)) for t in tqdm(train_texts))\n",
        "    print(f\"Max length: {max_len}\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
        "\n",
        "    print(\"\\nâœ‚ï¸ Tokenizing datasets...\")\n",
        "    tokenized = datasets.map(tokenize_fn, batched=True, remove_columns=[\"Text\"])\n",
        "    tokenized = tokenized.rename_column(\"Label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[ \"input_ids\", \"attention_mask\",\"labels\"])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# === 4. DataLoader ===\n",
        "def create_loaders(datasets):\n",
        "    return {\n",
        "        split: DataLoader(ds, batch_size=BATCH_SIZE,  shuffle=(split == \"train\"),num_workers=NUM_WORKERS)\n",
        "        for split, ds in datasets.items()\n",
        "    }\n",
        "\n",
        "\n",
        "# === ä¸»æµç¨‹ ===\n",
        "print(\"ğŸ— Preparing data...\")\n",
        "train_df, val_df, test_df = prepare_data()\n",
        "datasets = build_datasets(train_df, val_df, test_df)\n",
        "loaders = create_loaders(datasets)\n",
        "\n",
        "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"validation\"], loaders[\"test\"]\n",
        "\n",
        "print(\"\\nâœ… Data pipeline ready!\")\n",
        "for name, loader in loaders.items():\n",
        "    print(f\"{name:>10}: {len(loader)} batches\")\n",
        "\n",
        "# === ç¤ºä¾‹è¾“å‡º ===\n",
        "print(\"\\nğŸ“¦ Example batch from train_loader:\")\n",
        "batch = next(iter(loaders[\"train\"]))\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k:>15}:\", v.shape)\n",
        "\n",
        "# æŸ¥çœ‹å®é™…æ–‡æœ¬\n",
        "print(\"\\nğŸ“ Decoded sample text:\")\n",
        "sample_ids = batch[\"input_ids\"][0]\n",
        "decoded = GPT2Tokenizer.from_pretrained(\"gpt2\").decode(sample_ids, skip_special_tokens=True)\n",
        "print(decoded)\n",
        "print(\"Label:\", batch[\"labels\"][0].item())\n"
      ],
      "metadata": {
        "id": "ntTbiSLy4sGi"
      },
      "id": "ntTbiSLy4sGi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "KTYb7GO9SWjd"
      },
      "id": "KTYb7GO9SWjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¨¡å‹ä¸è®­ç»ƒ"
      ],
      "metadata": {
        "id": "mT-AD-VLUrfI"
      },
      "id": "mT-AD-VLUrfI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6093a7ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6093a7ed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "class GPT2ClassificationModel(nn.Module):\n",
        "  def __init__(self,num_labels = 2):\n",
        "    super().__init__()\n",
        "    self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "    config = self.gpt2.config\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_labels, bias=True)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask):\n",
        "    gpt2_out = self.gpt2(input_ids,attention_mask=attention_mask)\n",
        "    logits = self.classifier(gpt2_out.last_hidden_state[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "def train(model, train_loader,val_loader, optimizer,loss_fn, lr_scheduler, device,progress_bar):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(outputs, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    # 3. è¯„ä¼°å¾ªç¯\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0  # åˆå§‹åŒ–æ€»æŸå¤±\n",
        "\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"])\n",
        "            loss = loss_fn(logits, batch[\"labels\"])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "        total_samples += len(batch[\"labels\"])\n",
        "\n",
        "    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "    avg_val_loss = total_loss / len(val_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = GPT2ClassificationModel(num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# --- æ”¯æŒï¼šå†»ç»“ GPT2 æ¨¡å‹çš„å‚æ•°ï¼Œæˆ–è€…åªè®­ç»ƒæŸå‡ å±‚ ---\n",
        "# for param in model.gpt2.parameters():\n",
        "#     param.requires_grad = False\n",
        "# ------------------------------------\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 2. è®­ç»ƒå¾ªç¯\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "train(\n",
        "    model = model,\n",
        "    train_loader=train_loader ,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device,\n",
        "    progress_bar=progress_bar\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##éªŒè¯\n"
      ],
      "metadata": {
        "id": "gHT_7IXUUk1W"
      },
      "id": "gHT_7IXUUk1W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æµ‹è¯•"
      ],
      "metadata": {
        "id": "Z4m62S0_UwC_"
      },
      "id": "Z4m62S0_UwC_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBjVhZLuwRT"
      },
      "id": "ZOBjVhZLuwRT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}